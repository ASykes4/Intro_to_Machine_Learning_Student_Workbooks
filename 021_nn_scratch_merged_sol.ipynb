{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import accuracy_score\n",
    "import pandas as pd\n",
    "\n",
    "from keras.datasets import mnist\n",
    "from keras.utils import np_utils\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Network Basics\n",
    "\n",
    "We can try to build a neural network from scratch to see the details of how it works. We won't be using these homemade networks for real purposes, but it is good to get a bit of a look at the mechanics to help make things make sense. \n",
    "\n",
    "Scroll down to the diabetes section to look at what we are going to create to solve a problem that we are familiar with - logistic regression. \n",
    "\n",
    "## What is a Neural Network?\n",
    "\n",
    "Neural networks are a type of machine learning algorithm that is inspired by the way that the human brain works. The human brain is made up of neurons that are connected to each other, and a neural network aims to replicate that. We have inputs, output(s), and some number of layers in between. This sounds complex, and it can be, but we can break it down into a few simple steps.\n",
    "\n",
    "![Neural Network](images/nn_structure.png \"Neural Network\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helpers - Base Layer and Support Functions\n",
    "\n",
    "First we need a few helper functions - separating these out will make our code much easier to read. Below we have:\n",
    "<ul>\n",
    "<li> Base Layer Class: we are going to have two types of layers - the \"normal\" fully connected one, and an activation one to apply the activation function. In the future the activation is wrapped into the normal layer, but this will help us see the parts more clearly. \n",
    "<li> Activation Functions and Derivitives: we have two activation functions, and the derivitive of each. We'll talk about different activation functions in more detail later on. \n",
    "<li> Loss Function and Derivitive: we have the loss function and its derivitive. We'll keep it simple and use MSE. We'll also talk more about different loss functions later on. \n",
    "<li> Convert to Bool: this just translates probability predictions [0,1] into a binary classification. We just need it to calculate predicted accuracy of our test data predictions. \n",
    "</ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Layer:\n",
    "    def __init__(self):\n",
    "        self.input = None\n",
    "        self.output = None\n",
    "\n",
    "    # computes the output Y of a layer for a given input X\n",
    "    def forward_propagation(self, input):\n",
    "        raise NotImplementedError\n",
    "\n",
    "    # computes dE/dX for a given dE/dY (and update parameters if any)\n",
    "    def backward_propagation(self, output_error, learning_rate):\n",
    "        raise NotImplementedError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# activation function and its derivative\n",
    "def tanh(x):\n",
    "    return np.tanh(x)\n",
    "\n",
    "def tanh_prime(x):\n",
    "    return 1-np.tanh(x)**2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# activation function and its derivative\n",
    "def sigmoid(z):\n",
    "    s = 1 / (1 + np.exp(-z))\n",
    "    return s\n",
    "\n",
    "def sigmoid_prime(z):\n",
    "  fz = sigmoid(z)\n",
    "  return fz * (1 - fz)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loss function and its derivative\n",
    "def mse(y_true, y_pred):\n",
    "    return np.mean(np.power(y_true-y_pred, 2))\n",
    "\n",
    "def mse_prime(y_true, y_pred):\n",
    "    return 2*(y_pred-y_true)/y_true.size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert a decimal between 0 and 1 to either 0 or 1, based on if it is\n",
    "# above or below the cutoff. \n",
    "def conv_to_bool(float_list, cutoff=.5):\n",
    "    new_list = []\n",
    "    for i in float_list:\n",
    "        if i < cutoff:\n",
    "            new_list.append(0)\n",
    "        else:\n",
    "            new_list.append(1)\n",
    "    return new_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Usable Layers\n",
    "\n",
    "We can create the layers that we'll actually use now. Normally the FClayer and the activation layer we are creating would be combined, but this makes it easier to explore.\n",
    "\n",
    "#### Fully Connected Layer (Dense)\n",
    "\n",
    "The main component of the neural network is the fully connected, or dense, layer. The fully connected (dense) part just means that each neuron is connected to each neuron in the next layer. The three functions each have simple jobs:\n",
    "\n",
    "<ul>\n",
    "<li> Initialization: set up the matrix of weights and a vector of bias values. Here they are initialized to a random bumber, in practice these are initialized using some smart method, configurable with a parameter. Each neuron connects to each other neuron, so there is a wight for each of those connections - a matrix of input size by output size. Each output has one bias value. \n",
    "<li> Forward Propagation: in forward propagation this layer generates predictions by multiplying weights * values and adding the bias. The calculation itself is a dot product - which multiplies each input by its corresponding weight in the matrix automatically. This could be done by lots of loops, but this is more compact and efficient. Recall that each connection has a weight, so we end up with this large matrix. \n",
    "<li> Backward Propagation: this layer calculates the impact of the error stemming from each input X. This is done by calculating the gradient of the loss with respect to each input - thus allowing us to attribute portions of the output error to the different inputs. The calculation is done using the derivitive of the activation function and some math called the chain rule. Once we have this, we update all the weights and the bias so that we shrink our loss. \n",
    "</ul>\n",
    "\n",
    "![Weights](images/weights.png \"Weights\" )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# inherit from base class Layer\n",
    "class FCLayer(Layer):\n",
    "    # input_size = number of input neurons\n",
    "    # output_size = number of output neurons\n",
    "    def __init__(self, input_size, output_size):\n",
    "        self.weights = np.random.rand(input_size, output_size) - 0.5\n",
    "        self.bias = np.random.rand(1, output_size) - 0.5\n",
    "\n",
    "    # returns output for a given input\n",
    "    def forward_propagation(self, input_data):\n",
    "        self.input = input_data\n",
    "        self.output = np.dot(self.input, self.weights) + self.bias\n",
    "        return self.output\n",
    "\n",
    "    # computes dE/dW, dE/dB for a given output_error=dE/dY. Returns input_error=dE/dX.\n",
    "    def backward_propagation(self, output_error, learning_rate):\n",
    "        input_error = np.dot(output_error, self.weights.T)\n",
    "        weights_error = np.dot(self.input.T, output_error)\n",
    "        \n",
    "        # update parameters\n",
    "        # dBias = output_error\n",
    "        self.weights -= learning_rate * weights_error\n",
    "        self.bias -= learning_rate * output_error\n",
    "\n",
    "        return input_error"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Activation Functions\n",
    "\n",
    "One of the most important parts of a neural network is the activation function. The activation function is what allows the network to learn non-linear relationships. Without an activation function, the network would be no more powerful than a linear regression. The activation function is applied to the output of each neuron, and it is what determines whether the neuron is \"on\" or \"off\". Prior to the output of each neuron being sent on to the next layer, the activation function is applied. If you're paying close attention, the output going into an activation function is just a linear regression - the sum of the features times their weight. The activation function changes this, and allows the neural network to be \"more\" than a complicated linear regression. In real applications, we usually have a few layers in our neural network, and the combination of multiple layers and activation functions allows us to learn very complex relationships. The term \"deep learning\" refers to networks that have lots of layers. \n",
    "\n",
    "![NN Steps](images/nn_steps.jpeg \"NN Steps\" )\n",
    "\n",
    "There are many different activation functions, including the one that we are used to - the sigmoid. These activation functions are things that we can experiment with and tune like our hyperparameters. Some tend to be better suited for different types of problems, such as ReLU being common for image work and tanh being common for text processing, but there is no definitive answer. There are also considerations for computational efficiency, and some activation functions are more computationally expensive than others. For the most part we will start with the most common one for the type of problem we are doing, and then experiment with others if we feel the need. We'll look at these more later. \n",
    "\n",
    "![Activation Functions](images/activation-functions.png \"Activation Functions\")\n",
    "\n",
    "#### Activation Layer\n",
    "\n",
    "The activation layer is more simple, it just applies the activation function. Normally this is built into the other layer, but this is a little more simple to build by hand. \n",
    "\n",
    "<ul>\n",
    "<li> Initialization: set the activation function and derivitive to use going forward. \n",
    "<li> Forward Propagation: in forward propagation the activation layer just takes the input that it gets and applies the activation function to it.  \n",
    "<li> Backward Propagation: in backward propagation the activation layer translates the error of the predictions back up by multiplying the error by the derivitive of the activation function. This has the effect of translating the error we got with respect to the output of the activation function into error with respect to the input of the activation function. \n",
    "</ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# inherit from base class Layer\n",
    "class ActivationLayer(Layer):\n",
    "    def __init__(self, activation, activation_prime):\n",
    "        self.activation = activation\n",
    "        self.activation_prime = activation_prime\n",
    "\n",
    "    # returns the activated input\n",
    "    def forward_propagation(self, input_data):\n",
    "        self.input = input_data\n",
    "        self.output = self.activation(self.input)\n",
    "        return self.output\n",
    "\n",
    "    # Returns input_error=dE/dX for a given output_error=dE/dY.\n",
    "    # learning_rate is not used because there is no \"learnable\" parameters.\n",
    "    def backward_propagation(self, output_error, learning_rate):\n",
    "        return self.activation_prime(self.input) * output_error"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Full Network\n",
    "\n",
    "We can now build our layers into an actual model. Our code here is a little long, but most of it is pretty simple - the majority of the extra stuff is to make the model flexible. \n",
    "\n",
    "<ul>\n",
    "<li> Add - call this to add layers to the model. \n",
    "<li> Use - provide a loss function, and the derivitive of a loss function for the model to use. \n",
    "<li> Predict - generate prediction. This is just one execution of a forward propagation for the new inputs. \n",
    "<li> Fit - train the model. \n",
    "    <ul>\n",
    "    <li> Each iteration is called an epoch. We loop through the process until we've hit the desired number of epochs. \n",
    "    <li> Run each sample through a forward propagation. \n",
    "    <li> Get the prediction, and calculate the error. \n",
    "    <li> Pass the error to the back propagation. Repeat\n",
    "    </ul>\n",
    "</ul>\n",
    "\n",
    "Another way to phrase the fitting and optimizing of the model is to think of the errors on each epoch. First we generate a prediction (FP), and find the error of that prediction by simply comparing it to the true value - this is the error with respect to y, the output. \n",
    "\n",
    "### Back Propagation and Gradient Descent\n",
    "\n",
    "Next we do the back propagation, this takes that error with respect to y that we figured out, and \"breaks\" that error down into error with respect to each term from the previous layer. Recall that one neuron creates a prediction (during FP) that is equal to w1*x1 + w2*x2 +... b; this does a sort of reverse execution of that - starting with the overall error and mapping it to each term. The error is effectively split into parts and each of the m*x terms and the b term is labeled as being accountable for some of it. Since the x values are the inputs, they can't change, so we modify the weights and bias to lessen the error - this is the gradient descent part. This gets repeated back through each layer until the first layer, then another FP begins with the new updated weights. The learning rate controls the size of the adjustments at each round.\n",
    "\n",
    "![Back Propagation](images/backprop.webp \"Back Propagation\" )\n",
    "\n",
    "This back propagation part is the key to the high accuracy ceiling that neural networks have when the data is large - we can tune each of many neurons specifically to their contribution of error. If we have enough data, we can get very accurate predictions. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Network:\n",
    "    def __init__(self):\n",
    "        self.layers = []\n",
    "        self.loss = None\n",
    "        self.loss_prime = None\n",
    "\n",
    "    # add layer to network\n",
    "    def add(self, layer):\n",
    "        self.layers.append(layer)\n",
    "\n",
    "    # set loss to use\n",
    "    def use(self, loss, loss_prime):\n",
    "        self.loss = loss\n",
    "        self.loss_prime = loss_prime\n",
    "\n",
    "    # predict output for given input\n",
    "    def predict(self, input_data):\n",
    "        # sample dimension first\n",
    "        samples = len(input_data)\n",
    "        result = []\n",
    "\n",
    "        # run network over all samples\n",
    "        for i in range(samples):\n",
    "            # forward propagation\n",
    "            output = input_data[i]\n",
    "            for layer in self.layers:\n",
    "                output = layer.forward_propagation(output)\n",
    "            result.append(output)\n",
    "\n",
    "        return result\n",
    "\n",
    "    # train the network\n",
    "    def fit(self, x_train, y_train, epochs, learning_rate):\n",
    "        # sample dimension first\n",
    "        samples = len(x_train)\n",
    "\n",
    "        # training loop\n",
    "        errors = []\n",
    "        for i in range(epochs):\n",
    "            err = 0\n",
    "            for j in range(samples):\n",
    "                # forward propagation\n",
    "                output = x_train[j]\n",
    "                for layer in self.layers:\n",
    "                    output = layer.forward_propagation(output)\n",
    "\n",
    "                # compute loss (for display purpose only)\n",
    "                #print(self.loss)\n",
    "                err += self.loss(y_train[j], output)\n",
    "\n",
    "                # backward propagation\n",
    "                error = self.loss_prime(y_train[j], output)\n",
    "                for layer in reversed(self.layers):\n",
    "                    error = layer.backward_propagation(error, learning_rate)\n",
    "\n",
    "            # calculate average error on all samples\n",
    "            if i % 100 == 0:\n",
    "                err /= samples\n",
    "                errors.append(err)\n",
    "                print('epoch %d/%d   error=%f' % (i+1, epochs, err))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use Network\n",
    "\n",
    "We have a neural network now - we need to use it! Since we were smart and made the framework able to handle input data of an arbitrary size, we can use our model for pretty much any application!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Diabetes Data\n",
    "\n",
    "One weird thing that we will do to the data here is that we will reshape the Xs into a 3 dimensional array. This is due to the structure of the model - it is expecting data to be in that shape. When we start using Keras (the package we use for neural network models) next week this can generally be handled automatically, but since we've made this from scratch, we need to do it here. \n",
    "\n",
    "What we end up with for the data is:\n",
    "<ul>\n",
    "<li> 8 numerical features. We will have 8 neurons / Xs in the input of our network. \n",
    "<li> 1 binary categorical output. We will have one neuron for the output of the network. \n",
    "<li> Activation function applied to our output, to ensure we get categorization. \n",
    "</ul>\n",
    "\n",
    "![SimpleNN](images/simple_nn.png \"SimpleNN\" )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Pregnancies</th>\n",
       "      <th>Glucose</th>\n",
       "      <th>BloodPressure</th>\n",
       "      <th>SkinThickness</th>\n",
       "      <th>Insulin</th>\n",
       "      <th>BMI</th>\n",
       "      <th>DiabetesPedigreeFunction</th>\n",
       "      <th>Age</th>\n",
       "      <th>Outcome</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>6</td>\n",
       "      <td>148</td>\n",
       "      <td>72</td>\n",
       "      <td>35</td>\n",
       "      <td>0</td>\n",
       "      <td>33.6</td>\n",
       "      <td>0.627</td>\n",
       "      <td>50</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>85</td>\n",
       "      <td>66</td>\n",
       "      <td>29</td>\n",
       "      <td>0</td>\n",
       "      <td>26.6</td>\n",
       "      <td>0.351</td>\n",
       "      <td>31</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>8</td>\n",
       "      <td>183</td>\n",
       "      <td>64</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>23.3</td>\n",
       "      <td>0.672</td>\n",
       "      <td>32</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>89</td>\n",
       "      <td>66</td>\n",
       "      <td>23</td>\n",
       "      <td>94</td>\n",
       "      <td>28.1</td>\n",
       "      <td>0.167</td>\n",
       "      <td>21</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>137</td>\n",
       "      <td>40</td>\n",
       "      <td>35</td>\n",
       "      <td>168</td>\n",
       "      <td>43.1</td>\n",
       "      <td>2.288</td>\n",
       "      <td>33</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Pregnancies  Glucose  BloodPressure  SkinThickness  Insulin   BMI  \\\n",
       "0            6      148             72             35        0  33.6   \n",
       "1            1       85             66             29        0  26.6   \n",
       "2            8      183             64              0        0  23.3   \n",
       "3            1       89             66             23       94  28.1   \n",
       "4            0      137             40             35      168  43.1   \n",
       "\n",
       "   DiabetesPedigreeFunction  Age  Outcome  \n",
       "0                     0.627   50        1  \n",
       "1                     0.351   31        0  \n",
       "2                     0.672   32        1  \n",
       "3                     0.167   21        0  \n",
       "4                     2.288   33        1  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"data/diabetes.csv\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_set_x shape: (576, 1, 8)\n",
      "train_set_y shape: (576, 1)\n",
      "test_set_x shape: (192, 1, 8)\n",
      "test_set_y shape: (192, 1)\n",
      "[[0.         0.66331658 0.63934426 0.         0.         0.4828614\n",
      "  0.1322774  0.        ]]\n",
      "[0]\n",
      "[[0.         0.64824121 0.6557377  0.         0.         0.46497765\n",
      "  0.26498288 0.13333333]]\n",
      "[0]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "y = np.array(df[\"Outcome\"]).reshape(-1,1)\n",
    "X = df.drop(columns={\"Outcome\"})\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y)\n",
    "scaler = MinMaxScaler()\n",
    "X_train = scaler.fit_transform(X_train).reshape(-1,1,8)\n",
    "X_test = scaler.transform(X_test).reshape(-1,1,8)\n",
    "\n",
    "#Look at some details\n",
    "print (\"train_set_x shape: \" + str(X_train.shape))\n",
    "print (\"train_set_y shape: \" + str(y_train.shape))\n",
    "print (\"test_set_x shape: \" + str(X_test.shape))\n",
    "print (\"test_set_y shape: \" + str(y_test.shape))\n",
    "print(X_test[0])\n",
    "print(y_test[0])\n",
    "print(X_train[0])\n",
    "print(y_train[0])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Implement Simple NN\n",
    "\n",
    "We can create the network that we defined above. You may also recognize this 1 layer model as a logistic regression:\n",
    "<ul>\n",
    "<li> The weights are the coefficients of the linear regression.\n",
    "<li> The bias is the intercept of the linear regression.\n",
    "<li> The activation function is the sigmoid function.\n",
    "<li> The features are the features. \n",
    "</ul>\n",
    "\n",
    "At our most simple, that is all a neural network is. The ones that are more capable and flexible are just more complex versions of this, normally with more layers. Each layer adds one of these logistic regression like steps, only they are being performed on the output of the previous layer. Multiple layers allow us to learn more complex relationships, as each layer can learn a different relationship from the transformed data. \n",
    "\n",
    "<b>Note:</b> We have to supply the \"sigmoid_prime\" and similar functions here. That's something that the library would normally do for us, but since we're building it from scratch, we have to do it ourselves. The prime version is what the nerual network uses to calculate the error in back propagation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1/1000   error=0.256416\n",
      "epoch 101/1000   error=0.243445\n",
      "epoch 201/1000   error=0.240179\n",
      "epoch 301/1000   error=0.237633\n",
      "epoch 401/1000   error=0.235225\n",
      "epoch 501/1000   error=0.232910\n",
      "epoch 601/1000   error=0.230685\n",
      "epoch 701/1000   error=0.228548\n",
      "epoch 801/1000   error=0.226497\n",
      "epoch 901/1000   error=0.224530\n",
      "0.6979166666666666\n"
     ]
    }
   ],
   "source": [
    "# Network\n",
    "net = Network()\n",
    "net.add(FCLayer(8, 1))\n",
    "net.add(ActivationLayer(sigmoid, sigmoid_prime))\n",
    "\n",
    "# Train\n",
    "net.use(mse, mse_prime)\n",
    "net.fit(X_train, y_train, epochs=1000, learning_rate=0.0001)\n",
    "\n",
    "# Evaluate on test set\n",
    "out = net.predict(X_test)\n",
    "pred_labels = conv_to_bool(out)\n",
    "print(accuracy_score(y_test, pred_labels))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Try a Larger Network\n",
    "\n",
    "What if we try adding some layers? Outside of the input and output the rest of the stuff inside our network is configurable. We can add whatever we want in terms of layer, and each layer can be any size, as long as the shape matches the previous and next layer. We can also try that other activation function if we want. What's the \"right\" size? That question doesn't have a direct answer, we'll look at some things we can use to estimate it ~ 2 workbooks from now. We can play around a bit with it now and see what we get. \n",
    "\n",
    "If our data isn't linearly separable, we should see our model more able to fit the data as we start adding layers.\n",
    "\n",
    "![MultiLayerNN](images/multi_layer_nn.jpeg \"MultiLayerNN\" )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1/1000   error=0.246727\n",
      "epoch 101/1000   error=0.232020\n",
      "epoch 201/1000   error=0.204289\n",
      "epoch 301/1000   error=0.160442\n",
      "epoch 401/1000   error=0.158763\n",
      "epoch 501/1000   error=0.157855\n",
      "epoch 601/1000   error=0.157285\n",
      "epoch 701/1000   error=0.156860\n",
      "epoch 801/1000   error=0.156490\n",
      "epoch 901/1000   error=0.156124\n",
      "0.796875\n"
     ]
    }
   ],
   "source": [
    "# Network\n",
    "net = Network()\n",
    "net.add(FCLayer(8, 50))            \n",
    "net.add(ActivationLayer(sigmoid, sigmoid_prime))\n",
    "net.add(FCLayer(50, 50))               \n",
    "net.add(ActivationLayer(sigmoid, sigmoid_prime))\n",
    "net.add(FCLayer(50, 50))               \n",
    "net.add(ActivationLayer(sigmoid, sigmoid_prime))\n",
    "net.add(FCLayer(50, 25))               \n",
    "net.add(ActivationLayer(sigmoid, sigmoid_prime))\n",
    "net.add(FCLayer(25, 1))  \n",
    "net.add(ActivationLayer(sigmoid, sigmoid_prime))\n",
    "\n",
    "# Train\n",
    "net.use(mse, mse_prime)\n",
    "net.fit(X_train, y_train, epochs=1000, learning_rate=0.01)\n",
    "\n",
    "# test on 3 samples\n",
    "out = net.predict(X_test)\n",
    "pred_labels = conv_to_bool(out)\n",
    "\n",
    "print(accuracy_score(y_test, pred_labels))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simple Example - XOR\n",
    "\n",
    "We can test and see what our network can do with a very simple trial. XOR is a logical operation - exclusive or. It is 1 only if exactly one input is 1, or else it is false. We have that data as the training X and y, we can see if our network can learn this very simple relationship before we test it on some real data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1/1000   error=0.705809\n",
      "epoch 101/1000   error=0.194032\n",
      "epoch 201/1000   error=0.007197\n",
      "epoch 301/1000   error=0.001927\n",
      "epoch 401/1000   error=0.001040\n",
      "epoch 501/1000   error=0.000696\n",
      "epoch 601/1000   error=0.000517\n",
      "epoch 701/1000   error=0.000408\n",
      "epoch 801/1000   error=0.000336\n",
      "epoch 901/1000   error=0.000284\n",
      "[array([[0.00081965]]), array([[0.9791605]]), array([[0.97658499]]), array([[-0.00132045]])]\n"
     ]
    }
   ],
   "source": [
    "# training data\n",
    "x_train = np.array([[[0,0]], [[0,1]], [[1,0]], [[1,1]]])\n",
    "y_train = np.array([[[0]], [[1]], [[1]], [[0]]])\n",
    "\n",
    "# network\n",
    "net = Network()\n",
    "net.add(FCLayer(2, 3))\n",
    "net.add(ActivationLayer(tanh, tanh_prime))\n",
    "net.add(FCLayer(3, 1))\n",
    "net.add(ActivationLayer(tanh, tanh_prime))\n",
    "\n",
    "# train\n",
    "net.use(mse, mse_prime)\n",
    "net.fit(x_train, y_train, epochs=1000, learning_rate=0.1)\n",
    "\n",
    "# test\n",
    "out = net.predict(x_train)\n",
    "print(out)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Complex Example - MNIST Images\n",
    "\n",
    "Now that we are at least a little comfortable with the usage of neural networks, we can start to apply it to more useful applications, such as our old friends, the MNIST digits. \n",
    "\n",
    "Here we will make a larger model and we can also try a differnet activation function. One other difference to note is the \"to_categorical\" applied to the target y values. This works with the final layer of 10 output neurons - we get a softmax type of output where there is a probability for being in each class. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 1, 784)\n",
      "epoch 1/1000   error=0.281474\n",
      "epoch 101/1000   error=0.060261\n",
      "epoch 201/1000   error=0.044899\n",
      "epoch 301/1000   error=0.034950\n",
      "epoch 401/1000   error=0.028376\n",
      "epoch 501/1000   error=0.023588\n",
      "epoch 601/1000   error=0.019862\n",
      "epoch 701/1000   error=0.016935\n",
      "epoch 801/1000   error=0.014624\n",
      "epoch 901/1000   error=0.012754\n",
      "\n",
      "\n",
      "predicted values : \n",
      "[array([[0.04250001, 0.02819593, 0.00245529, 0.04833485, 0.00259858,\n",
      "        0.07716673, 0.00191502, 0.9788831 , 0.00259579, 0.01005323]]), array([[0.40955815, 0.00411011, 0.83131782, 0.0183063 , 0.00306493,\n",
      "        0.27737266, 0.05305986, 0.00124992, 0.00972546, 0.00313835]]), array([[4.58886995e-04, 9.22404688e-01, 5.04474150e-02, 3.89158811e-02,\n",
      "        1.36515348e-03, 6.74213019e-03, 4.36350956e-02, 4.68242952e-02,\n",
      "        3.67121592e-02, 4.76676269e-02]])]\n",
      "true values : \n",
      "[[0. 0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      " [0. 0. 1. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "# load MNIST from server\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "\n",
    "# training data : 60000 samples\n",
    "# reshape and normalize input data \n",
    "x_train = x_train.reshape(x_train.shape[0], 1, 28*28)\n",
    "x_train = x_train.astype('float32')\n",
    "x_train /= 255\n",
    "# encode output which is a number in range [0,9] into a vector of size 10\n",
    "# e.g. number 3 will become [0, 0, 0, 1, 0, 0, 0, 0, 0, 0]\n",
    "y_train = np_utils.to_categorical(y_train)\n",
    "\n",
    "# same for test data : 10000 samples\n",
    "x_test = x_test.reshape(x_test.shape[0], 1, 28*28)\n",
    "x_test = x_test.astype('float32')\n",
    "x_test /= 255\n",
    "y_test = np_utils.to_categorical(y_test)\n",
    "\n",
    "print(x_train.shape)\n",
    "\n",
    "# Network\n",
    "net = Network()\n",
    "net.add(FCLayer(28*28, 100))                # input_shape=(1, 28*28)    ;   output_shape=(1, 100)\n",
    "net.add(ActivationLayer(tanh, tanh_prime))\n",
    "net.add(FCLayer(100, 50))                   # input_shape=(1, 100)      ;   output_shape=(1, 50)\n",
    "net.add(ActivationLayer(tanh, tanh_prime))\n",
    "net.add(FCLayer(50, 10))                    # input_shape=(1, 50)       ;   output_shape=(1, 10)\n",
    "net.add(ActivationLayer(sigmoid, sigmoid_prime))\n",
    "#net.add(FCLayer(50, 1)) \n",
    "\n",
    "# train on 1000 samples\n",
    "# we didn't use batches, which we'll look at more next time, so we can't use too much data or it will be slow. \n",
    "net.use(mse, mse_prime)\n",
    "net.fit(x_train[0:1000], y_train[0:1000], epochs=1000, learning_rate=0.001)\n",
    "\n",
    "# test on 3 samples\n",
    "out = net.predict(x_test[0:3])\n",
    "print(\"\\n\")\n",
    "print(\"predicted values : \")\n",
    "print(out, end=\"\\n\")\n",
    "print(\"true values : \")\n",
    "print(y_test[0:3])"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "4d722d3adfa415172c1f5238b519fb86b488acdae450fd691ab06c09f4ca9173"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 ('ml3950')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
