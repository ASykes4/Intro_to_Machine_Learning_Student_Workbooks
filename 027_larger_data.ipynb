{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-03-31 09:00:30.212515: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F AVX512_VNNI FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "  import google.colab\n",
    "  IN_COLAB = True\n",
    "except:\n",
    "  IN_COLAB = False\n",
    "\n",
    "if IN_COLAB:\n",
    "  !pip install wget\n",
    "  #!pip install split-folders\n",
    "  import wget\n",
    "  import zipfile\n",
    "  #import split-folders\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "from keras.layers import Dense, Flatten, Normalization, Dropout, Conv2D, MaxPooling2D, RandomFlip, RandomRotation, RandomZoom, BatchNormalization, Activation, InputLayer\n",
    "from keras.models import Sequential\n",
    "from keras.losses import SparseCategoricalCrossentropy, CategoricalCrossentropy\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.utils import np_utils\n",
    "from keras import utils\n",
    "import os\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Pipelines and Dealing with Larger Data Efficiently\n",
    "\n",
    "When dealing with larger amounts of data in our neural networks we need some tools to manage the data pipeline. We have started to look at a couple of these in the image generators and the datasets from directories. We can build on the dataset specifically to create data pipelines that both apply any data preparation steps and load our data efficiently. \n",
    "\n",
    "<b>Note:</b> in the project stuff I suggested using either generators or datasets for performance reasons, as I read more I found that the speed difference between the two has actually become really large, with datasets being much faster. We'll focus only on those here. Some articles said it is up to 30 times faster, which is pretty massive. Even if the difference isn't near that much, it is substantial, so the datasets will be more efficient for larger data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper to plot loss\n",
    "def plot_loss(history):\n",
    "  plt.plot(history.history['loss'], label='loss')\n",
    "  plt.plot(history.history['val_loss'], label='val_loss')\n",
    "  plt.legend()\n",
    "  plt.grid(True)\n",
    "  plt.show()\n",
    "\n",
    "def plot_acc(history):\n",
    "  plt.plot(history.history['accuracy'], label='accuracy')\n",
    "  plt.plot(history.history['val_accuracy'], label='val_accuracy')\n",
    "  plt.legend()\n",
    "  plt.grid(True)\n",
    "  plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download and Unzip Data\n",
    "ROOT_DIR = \"/content/simpsons_dataset\"\n",
    "\n",
    "def bar_custom(current, total, width=80):\n",
    "    print(\"Downloading: %d%% [%d / %d] bytes\" % (current / total * 100, current, total))\n",
    "\n",
    "zip_name = \"simpsons.zip\"\n",
    "\n",
    "url = \"https://jrssbcrsefilesnait.blob.core.windows.net/3950data1/simpsons.zip\"\n",
    "\n",
    "if not os.path.exists(zip_name):\n",
    "    wget.download(url, zip_name, bar=bar_custom)\n",
    "\n",
    "if not os.path.exists(\"/content/simpsons_dataset\"):\n",
    "    with zipfile.ZipFile(zip_name, 'r') as zip_ref:\n",
    "            zip_ref.extractall()\n",
    "            rm -rf \"simpsons_dataset/simpsons_dataset\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Parameters "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 8\n",
    "BUFFER = 150 \n",
    "VAL_SPLIT = .2\n",
    "IMG_SIZE = (256, 256, 3)\n",
    "IM_SIZE = (256, 256)\n",
    "EPOCHS = 10\n",
    "SEED = 123\n",
    "\n",
    "label_walk = os.walk(ROOT_DIR)\n",
    "labels = []\n",
    "for path, directories, files in label_walk:\n",
    "     for directory in directories:\n",
    "          labels.append(directory)\n",
    "\n",
    "print(labels)\n",
    "NUM_CLASSES = len(labels)\n",
    "print(NUM_CLASSES)\n",
    "\n",
    "checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\"logs/weights.{epoch:02d}-{val_loss:.2f}.hdf5\", monitor='val_accuracy', verbose=2, save_best_only=False, save_weights_only=True, mode='max', save_freq='epoch')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TF.Data Pipelines\n",
    "\n",
    "The data pipeline setup in tensorflow is a little different than what we are used to in sklearn. Here, the big thing that our pipeline is going to do for us is to offer greater efficiency. Neural networks are generally used with very large datasets that are too large to fint in memory, like a dataframe that we are used to using. The tensorflow pipeline works to pull the data off of disk and into memory in a way that efficiently uses resources. The pipeline that we create will do a better job of ensuring that data is being prepared while the model is training, and is prepped and ready to go without delay, which can really matter with large datasets. The way that it does this is to do a better job of batching and processing the data, to ensure that we waste as little time idle as possible. Recall that when training models we normally use the GPU (graphics processor) becuase it is way faster, but most of the work outside of the fit method still relies on the CPU. The data pipelines work to make sure that the CPU can process and prepare data in parallel with the GPU doing the fitting, so as soon as one batch is finished processing, another is ready to go. Less delay means more of the run time spent processing rather than waiting. \n",
    "\n",
    "<b>From:</b>\n",
    "![Data Pipelines](images/tf_data_idle.png \"Data Pipelines\")\n",
    "<b>To:</b>\n",
    "![Data Pipelines](images/tf_data_idle2.png \"Data Pipelines\")\n",
    "\n",
    "We have already used these datasets to load data, now we will look at some things that we can do to make the data pipeline more efficient when the data gets larger. These pipelines are a little more focused on the logistics of moving data around than on doing data cleanup like imputation that we often did in the sklearn stuff. When we are dealing with something like images, that type of data prep isn't really relevant. If we were applying this to structured data, cleanup steps could be applied through the .map() function, or we could just clean it before starting the modelling. \n",
    "\n",
    "### Construct the Datasets\n",
    "\n",
    "We start by making the dataset objects - here we will load in image data from a directory structure, there are other methods to create datasets from other stuff like arrays. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds = tf.keras.utils.image_dataset_from_directory(ROOT_DIR, validation_split=VAL_SPLIT, subset=\"training\", seed=SEED, batch_size=None)\n",
    "val_ds = tf.keras.utils.image_dataset_from_directory(ROOT_DIR, validation_split=VAL_SPLIT, subset=\"validation\", batch_size=None, seed=SEED)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup Preprocessing Pipeline\n",
    "\n",
    "Now that we have the basics of the dataset created, we can work on the pipeline to deliver that data in the form we want it. There are several things that we can do to our dataset, some that we'll focus on here are:\n",
    "<ul>\n",
    "<li> Cache - preload some data to speed the process. \n",
    "<li> Map - apply a function to all the data, usefull to apply transformations like normalization. \n",
    "<li> Batch - creates batches of data. \n",
    "<li> Prefetch - retreives data early to eliminate delays. \n",
    "<li> Shuffle - pulls data randomly to create batches. \n",
    "</ul>\n",
    "\n",
    "### Pipeline Syntax\n",
    "\n",
    "The creation of the entire pipeline is simple from a coding perspective, we just chain all of the functions that we are going to use onto the dataset. \n",
    "\n",
    "One key thing is that the things in the pipeline are done in order, sometimes this can matter. There are lots of potential combinations of actions that could be put together into a pipeline, so we can't make universal rules on the order, there are a few guidelines that we can use though:\n",
    "<ul>\n",
    "<li> Prefetch should be last. \n",
    "</ul>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Map and (Not) Batch\n",
    "\n",
    "Mapping works here like it works anywhere else, we can apply a function to the entire dataset. For dataprep, this is useful, as if we need to do a transformation we can build it into the pipeline here. A big thing here is that we want to work on the data with vectorized operations, not loops. This map function will apply itself to the data\n",
    "\n",
    "Batches work like we are used to, but we won't use them now. The image_dataset_from_directory batches the data itself, so if we batch it again here we end up with batches of batches, which makes the data we give to the model 5D instead of 4 - (batch_size1), (batch_size2), (height), (width), (color_depth). If we were loading data from something that didn't batch the data already, such as from a dataframe, we could enable this here. This is an error that is very annoying, take my word for it. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resize_and_rescale = tf.keras.Sequential([\n",
    "    tf.keras.layers.Resizing(IMG_SIZE[0], IMG_SIZE[1]),\n",
    "    tf.keras.layers.Rescaling(1./255)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds = train_ds.map(resize_and_rescale)\n",
    "train_ds = train_ds.batch(BATCH_SIZE)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cache and Shuffle\n",
    "\n",
    "Cache just pulls data into memory early, so there is less delay to load it. In addition, you can optionally specify a file location for the cache location - while this probably won't help us, in a server environment you may have a RAM Disk, which is exactly what it sounds like, so you could potentially cache from a regular disk to the super fast ram disk. Here we won't cache, as the dataset is huge and this causes out-of-memory errors on Colab - if we had more RAM we could do this.  \n",
    "\n",
    "Shuffle just randomizes the order of the data, the buffersize controls how many items are shuffled at once; here we've let the autotune manage that size. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train_ds = train_ds.cache()\n",
    "train_ds = train_ds.shuffle(buffer_size=BUFFER)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Prefetch\n",
    "\n",
    "Prefetch is what forces the system to do its data preparation work in parallel with the modelling work, so both the CPU and GPU can stay busy. We allow the autotune (below) to control how much is prepared in advance. This ensures that the training step of one batch, done in the GPU, is set to overlap with the processing and preparation of another batch, done by CPU. This means that the time between batches that the GPU is fitting is minimized, since we are loading and fitting many, many, many batches, making a small improvement here will really add up over the entire training process. \n",
    "\n",
    "## Autotune\n",
    "\n",
    "One lesser known fact about neural networks is that T-Pain has done a large amount of research into efficiently loading data, even developing a tool predicably named autotune. In addition to making the voice of T-Pain resemble that of an angel, autotune works to make our data pipeline more efficient by monitoring some metrics on performance as the dataset works, and automatically making adjustments to improve things. The ramp-up process for the autotune to learn can impose some performance penalties on the early steps of training as the algorithm is analyzing the data, but once it learns an optimal set of values the efficiency will improve. This makes the autotune tool good for larger and longer training times, as the inital tuning time will become negligable as training progresses. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TPAIN = tf.data.AUTOTUNE\n",
    "train_ds = train_ds.prefetch(buffer_size=TPAIN)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finishing Pipelines and Handling Datasets\n",
    "\n",
    "We can finish up by mirroring the steps above on our validation dataset. \n",
    "\n",
    "### Managing Resources\n",
    "\n",
    "One thing that we can do with these datasets is set the parameters to limit resource use. Resource usage can be monitored on colab by clicking the RAM/CPU icon towards the top right, on a computer you could use the activity manager, task manager, or any other program that monitors RAM usage. \n",
    "<ul>\n",
    "<li> GPU Ram is exceeded - likely can be addressed by the batch size and model size. One batch is processed at a time, so a smaller batch means less memory usage. It isn't the only factor, doing \"stuff\" (e.g. the matrix multiplication) with the massive matricies of weights also takes up memory, but that is somewhat harder to pinpoint other than smaller models use less RAM. \n",
    "<li> System RAM is exceeded - this could be many things, the most likely cause is that too much data was loaded in for some operation that causes the RAM usage to spike.\n",
    "    <ul>\n",
    "    <li> Shuffle and prefetch options both load more data at a time the higher the limit is, we can lower them to limit RAM usage. \n",
    "    </ul>\n",
    "</ul>\n",
    "\n",
    "Unfortunately the autotune won't assure us that memory use limits don't get hit. \n",
    "\n",
    "#### Memory Usage\n",
    "\n",
    "This is one of the (likely) few times we really need to monitor RAM usage. In general, your computer is capable of swapping - moving stuff in and out of RAM and back to disk on the fly to ensure everything works. Here, swapping data to disk is so much slower that it is essentially impossible, so if we put 16.1GB in 16GB of RAM, everything dies. The most likely culprits are things that try to do something to all the data at once, it is likely that with larger datasets such actions aren't even possible. We can use the .map() function in the dataset to apply things to the dataset in a managed way. There are models, like some of the larger image or NLP ones, that can't run on normal consumer hardware because the RAM requirements on the GPU are too large. We can see something similar on a smaller scale with smart robotic toys, the ones that use something like a Raspberry Pi or other small computer as a programmable brain. Some of these can be AI driven by loading a neural network model that was trained elsewhere (your laptop) on that makes the speed and steering decisions based on what it sees and senses; that model can only be so large though as the small single board computers are limited in their capacity. \n",
    "\n",
    "#### Performance Tuning\n",
    "\n",
    "Since training models is highly dependent on speed, both for practicality concerns and to be able to test more possible models, eliminating bottlenecks and tuning performance is a massive topic. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validation Stuff\n",
    "#val_ds = val_ds.cache()\n",
    "val_ds = val_ds.map(resize_and_rescale)\n",
    "val_ds = val_ds.batch(BATCH_SIZE)\n",
    "val_ds = val_ds.prefetch(buffer_size=TPAIN)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pulling Data\n",
    "\n",
    "Getting some example data out of our dataset is a little different because they aren't a basic data structure like a dataframe or an array, so we can't just say \"give me item 7\". We need to approach getting data from the dataset similarly to how it provides did to a fit method, we ask for some data and the dataset produces one batch for us. We can do this with the \"as_numpy_iterator\" method, which returns an iterator. The iterator, well, iterates over the dataset, so to get some more data we can ask it for the next() piece of data. \n",
    "\n",
    "The dataset .next() call returns a batch of data in a 5D array. The first dimension is data vs label, data is 0, labels are 1. The next is the individual item index, and the last 3 are the dimensions of the image - height x width x color depth. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grab Some Data\n",
    "some_data = train_ds.as_numpy_iterator()\n",
    "sample_data = some_data.next()\n",
    "plt.imshow(sample_data[0][0].astype(\"int\"))\n",
    "tmp_ind = sample_data[1][0]\n",
    "print(\"Label Index:\", tmp_ind, \" Label:\", labels[tmp_ind])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modelling\n",
    "\n",
    "Once the data pipelines are setup, using them is the same as always. Our datasets will handle all the things that we setup above all on their own, and will provide data to the fit method as it requires it. \n",
    "\n",
    "Since this training process may take a while, we will also write a checkpoint callback to save the weights every time we improve the model. The wonky stuff in the file name just assigns each set of weights saved with a label of their epoch and accuracy, a common way to log multiple sets of weights. \n",
    "\n",
    "### Advanced Activation\n",
    "\n",
    "Some of activation functions outside of the standards like ReLU require some special considerations. ReLU is the most commonly used activation function on the hidden layers, as it generally performs well and is computationally efficient. The Keras docs have a note in there that the \"advanced activation functions\" like PReLU and LeakyReLU should not be added as an argument to a regular layer, but should be added as their own layer. This is because these advanced activations have something that is learned in training. The end effect in the structure and functionality of the model doesn't change, it is simply to allow the way things are implemented in tensorflow to function correctly. If you recall the scratch-made neural network that we started with, those activations were were also added as a separate layer there.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Rescaling(1./255, input_shape=IMG_SIZE),\n",
    "    tf.keras.layers.Conv2D(64, (3,3), padding=\"same\", kernel_regularizer=\"l2\"),\n",
    "    tf.keras.layers.LeakyReLU(),\n",
    "    tf.keras.layers.MaxPooling2D(),\n",
    "    tf.keras.layers.Conv2D(128, (3,3), padding=\"same\", kernel_regularizer=\"l2\"),\n",
    "    tf.keras.layers.LeakyReLU(),\n",
    "    tf.keras.layers.MaxPooling2D(),\n",
    "    tf.keras.layers.Conv2D(256, (3,3), padding=\"same\", kernel_regularizer=\"l2\"),\n",
    "    tf.keras.layers.LeakyReLU(),\n",
    "    tf.keras.layers.MaxPooling2D(),\n",
    "    tf.keras.layers.Conv2D(512, (3,3), padding=\"same\", kernel_regularizer=\"l2\"),\n",
    "    tf.keras.layers.LeakyReLU(),\n",
    "    tf.keras.layers.MaxPooling2D(),\n",
    "    tf.keras.layers.Dropout(.2),\n",
    "\n",
    "    tf.keras.layers.Flatten(),\n",
    "    tf.keras.layers.Dense(512, kernel_regularizer=\"l2\"),\n",
    "    tf.keras.layers.LeakyReLU(),\n",
    "    tf.keras.layers.Dropout(.3),\n",
    "    tf.keras.layers.Dense(128, kernel_regularizer=\"l2\"),\n",
    "    tf.keras.layers.LeakyReLU(),\n",
    "    tf.keras.layers.Dense(NUM_CLASSES)\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(\n",
    "  optimizer=tf.optimizers.Adam(learning_rate=0.001),\n",
    "  loss=tf.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "  metrics=['accuracy'])\n",
    "\n",
    "log_m1 = model.fit(\n",
    "    train_ds,\n",
    "    validation_data=val_ds,\n",
    "    epochs=EPOCHS,\n",
    "    callbacks=[checkpoint_callback]\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict the label of the test_images\n",
    "pred = model.predict(val_ds)\n",
    "pred = np.argmax(pred,axis=1)\n",
    "\n",
    "# Map the label\n",
    "labels = (train_ds.class_indices)\n",
    "labels = dict((v,k) for k,v in labels.items())\n",
    "pred = [labels[k] for k in pred]\n",
    "\n",
    "# Display the result\n",
    "print(f'The first 5 predictions: {pred[:5]}')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setting up a Pipeline\n",
    "\n",
    "These pipelines result in a balance of memory usage vs. speed - we can load a bunch more stuff in memory to make it faster, or we can endure losses in speed to save memory. Ideally, in a real world setting, we have some system (real or virtual) that we are using to train our models and we want to set things up to utilize those resources as fully as possible. On colab, the RAM we are allocated isn't massive, so we may not be able to lean really heavily on some of the functionality here to preload our data. If we were doing a similar task in a \"real\" scenario, it would be pretty easy for us to allocate or buy something like 64GB of RAM and really utilize it to ensure that both the CPU and GPU have work to do all the time. \n",
    "\n",
    "Since RAM is likely the limiting factor when we are using colab, configuration of the pipeline also allows us to tailor the memory usage of our model to fit the constraints. For example, this is a snapshot of the RAM usage of this code with the buffer for shuffle set to 8, the prefetch suing autotune, and the batch size set to 8. We could bump these up a bit, and since the resource usage tends to flatten out once everything starts up it won't be too difficult to get our resource usage pretty close to the limit. \n",
    "\n",
    "![RAM Usage](images/flat_ram_usage.png \"Flat Ram\")\n",
    "\n",
    "When I increased the shuffle buffer to 250, the resources now look like:\n",
    "\n",
    "![RAM Usage](images/flat_ram_usage2.png \"Flat Ram\")\n",
    "\n",
    "Maximizing the batch size when there is a lot of data to process will generally make the training process faster as there is less wasted capacity in the GPU on each batch. For larger datasets this is probably a larger concern than what we saw when we first touched on batch size - that we likely want it small for the best results. Even if a smaller batch performs better in a vacuum, faster epochs mean we can train and experiment more, which will likely outweigh any improvements that a small batch may bring. If we are dealing with images, a \"large batch\" probably won't be that large anyway. So, tl;dr, if using a large and slow model, fill the GPU with the largest batch it can take. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mapping Augmentation\n",
    "\n",
    "We can cycle back to the .map() part and now try to reconstruct some datasets that use data augmentation. To do so we will make a mini-model that just does the augmentation steps, then build that into a little function that we can apply to all the data. One reminder here, the augmentation should only be done on the training data, and we don't really need to shuffle the validation data either, so we can build those steps in as optional. \n",
    "\n",
    "<b>Note:</b> this could probably act as a pretty simple template for things that you may want to map to a dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_augmentation = tf.keras.Sequential([\n",
    "  tf.keras.layers.RandomFlip(\"horizontal_and_vertical\"),\n",
    "  tf.keras.layers.RandomRotation(0.2),\n",
    "])\n",
    "\n",
    "def prepare(ds, autotune, shuffle=False, augment=False, shuff_size=200, cache=False):\n",
    "  if cache:\n",
    "    ds = ds.cache()\n",
    "  # Resize and rescale all datasets.\n",
    "  ds = ds.map(lambda x, y: (resize_and_rescale(x), y), num_parallel_calls=autotune)\n",
    "\n",
    "  if shuffle:\n",
    "    ds = ds.shuffle(shuff_size)\n",
    "\n",
    "  # Use data augmentation only on the training set.\n",
    "  if augment:\n",
    "    ds = ds.map(lambda x, y: (data_augmentation(x, training=True), y), num_parallel_calls=autotune)\n",
    "\n",
    "  # Use buffered prefetching on all datasets.\n",
    "  return ds.prefetch(buffer_size=autotune)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Go Pro\n",
    "\n",
    "I have a Colab Pro subscription, which gives some access to better resources. For this, I am going to setup a little switch to use that with one easy change. When I make these changes we should expect much more speed - the entire dataset is held in memory rather than disk, and the batch size that we process each step is much larger. The costs are not small though:\n",
    "\n",
    "![High Resources](images/high_resource.png \"High Resources\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "HIGH_RESOURCE = False\n",
    "CACHE = False\n",
    "\n",
    "if HIGH_RESOURCE:\n",
    "    BATCH_SIZE = 64\n",
    "    CACHE = True\n",
    "\n",
    "train_aug = tf.keras.utils.image_dataset_from_directory(ROOT_DIR, validation_split=VAL_SPLIT, subset=\"training\", seed=SEED, batch_size=BATCH_SIZE, shuffle=True, image_size=IM_SIZE)\n",
    "val_aug = tf.keras.utils.image_dataset_from_directory(ROOT_DIR, validation_split=VAL_SPLIT, subset=\"validation\", batch_size=BATCH_SIZE, seed=SEED, shuffle=True, image_size=IM_SIZE)\n",
    "at2 = tf.data.AUTOTUNE\n",
    "\n",
    "train_aug = prepare(train_aug, at2, shuffle=True, augment=True, cache=CACHE)\n",
    "val_aug = prepare(val_aug, at2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model2 = tf.keras.Sequential([\n",
    "    tf.keras.layers.Conv2D(128, (3,3), padding=\"same\", kernel_regularizer=\"l2\", input_shape=IMG_SIZE),\n",
    "    tf.keras.layers.LeakyReLU(),\n",
    "    tf.keras.layers.MaxPooling2D(),\n",
    "    tf.keras.layers.Conv2D(512, (3,3), padding=\"same\", kernel_regularizer=\"l2\"),\n",
    "    tf.keras.layers.LeakyReLU(),\n",
    "    tf.keras.layers.MaxPooling2D(),\n",
    "    tf.keras.layers.Dropout(.2),\n",
    "\n",
    "    tf.keras.layers.Flatten(),\n",
    "    tf.keras.layers.Dense(128, kernel_regularizer=\"l2\"),\n",
    "    tf.keras.layers.LeakyReLU(),\n",
    "    tf.keras.layers.Dense(NUM_CLASSES)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model2.compile(\n",
    "  optimizer=tf.optimizers.Adam(),\n",
    "  loss=tf.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "  metrics=['accuracy'])\n",
    "\n",
    "log_m2 = model2.fit(\n",
    "    train_aug,\n",
    "    validation_data=val_aug,\n",
    "    epochs=EPOCHS,\n",
    "    callbacks=[checkpoint_callback]\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Local Minima - a Happy Accident!\n",
    "\n",
    "While I was putting this together I had one run that gave a really dramatic example of a local minima in the loss. Look at the loss value through the epochs here - our loss gets small, then explodes, then starts to get smaller again (though we don't have enough epochs here to see where we end up):\n",
    "\n",
    "![Local Minima](images/local_min.png \"Local Minima\")\n",
    "\n",
    "There is a pretty good chance our visualized gradient descent process looked something like this:\n",
    "\n",
    "![Local Min](images/local_min_nn.png \"Local Minima\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Automated Machine Learning\n",
    "\n",
    "Many of the things that we need to do to create ML models is somewhat automatable, particularly the parts around fitting and tuning a model. Experimenting with things like number of neurons in a neural network or different combinations of hyperparameters for any model is something that can be automated and many tools exist to do this. The goal of these tools is generally to make the process of creating machine learning models more accessible to people who don't have a lot of experience with the process. Current tools, and likely ones that will exist in the near future, aren't smart enough to totally replace all the work we need to do, but they can replace parts of it and they are getting better all the time. This is conjecture, but I'd anticipate that true breakthroughs in the capabilities of automated machine learning will explode as more and more services are moved to cloud services. The corporate market, filled with large amounts of data and few people who really analyze it, makes for a promising market, and the semi-standardization in the data that will come as more customers use cloud tools offered by one a few providers (AWS, Azure, Google) solves one of the biggest challenges to automating the process - variation. Companies forecasting sales, predicing loan defaults, or segmenting customers are all doing the same fundamental task, if the data is highly consistent coming out of the same systems, it makes it much easier to design an automated ML workflow that needs little to no human intervention. The machine learning bits can just be another service that the cloud providers can sell to their clients. \n",
    "\n",
    "These automated ML systems are normally a nice and friendly wrapper around the same core libraries that we use to build things from scratch. \n",
    "\n",
    "### AutoKeras \n",
    "\n",
    "We can take a peek at one automated machine learning library, AutoKeras. We won't spend a lot of time on it as it doesn't really do anything new or different, it just does the work for us. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if IN_COLAB:\n",
    "    !pip install autokeras\n",
    "    import autokeras as ak"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model\n",
    "\n",
    "We can create the model, it should be easy. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = ak.ImageClassifier(overwrite=True, max_trials=1)\n",
    "# Feed the tensorflow Dataset to the classifier.\n",
    "clf.fit(train_ds, validation_data=val_ds, epochs=EPOCHS)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bob's Your Uncle\n",
    "\n",
    "The automation does save us the time of making a model, deciding on a structure, and picking hyperparameters, but that definitely isn't all the work and I'd say not really the hardest part of the process. The work in getting and prepping the data to go into the model, automated or not, is probably more challenging and complex than making a model to do the predicting. If you're designing complex models to do chatGPTesque breakthroughs this may not hold true, but these automodels aren't doing anything that complex. This is why I think that automated machine learning will be most useful in the corporate world, where the data is (often) highly consistent and the models are relatively simple."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml3950",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
