{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Large Structured Data\n",
    "\n",
    "When dealing with large amounts of structured text data we can also do some stuff to speed things up, though there are some key differences that lessen our toolkit:\n",
    "<ul>\n",
    "<li> Data manipulation (filtering out features, customized data cleanup, etc...) is far easier to do in a tabular format like a dataframe. If there is going to be a lot of that, and the data is really large, we can do the 'manual' prep separately, write the data to a file, and then read it in again as ready-to-use data. </li>\n",
    "<li> As the data gets really large, most real life scenarios will either use big data approaches like Spark, or store structured data in a DB. That's the 'real' way to deal with large amounts of strucutred data, so there are not as many easy to use tools for this as we find with images. </li>\n",
    "<li> Further to the two ponts above, if the dataset is a CSV, we can likely load it into memory in its entirety as \"too many rows to fit in memory\" and \"this data is stored in a CSV file\" tend not to come around together all that often in a situation where there is actual infrastructure. </li>\n",
    "<li> Really large amounts of text can be broken into multiple smaller files, then we load a file at a time, similar to how we deal with images. This is common with NLP text, much more so than structured data. </li>\n",
    "<li> There is often an assumption that when needing to deal with large amounts of structured CSV data that we have the data already split into training and validation sets. This makes sense, as \n",
    "</ul>\n",
    "\n",
    "On the whole, dealing with large amounts of structured data tends to not be as large of an issue to be solved as dealing with large amounts of unstrucutured data in a non-big data environment. This is because huge data goes to big data strategies, or at least a DB, less huge data can just fit in memory and be dealt with how we have dealt with all other CSV based data. \n",
    "\n",
    "## TensorFlow Datasets\n",
    "\n",
    "Tensorflow Datasets are something that we used when loading image files from disk, as loading all of the data at once can be impossible for larger datasets. These datasets serve the same function as a regular dataframe for modle training purposes, but they are designed more to be able to efficiently load large amounts of data from disk than to allow easy viewing and manipulation of the data. \n",
    "\n",
    "Tensorflow datasets allow us to set several options on how the data is loaded, that we can use to make a dataset that is more efficient for our purposes."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset for Structured CSV\n",
    "\n",
    "The function below reads CSV data from disk and generates training and validation datasets that we can feed to our model. We also add batching, shuffle the training data, and use prefetch to make the data loading process more efficient."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import csv\n",
    "import time\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from sklearn.model_selection import train_test_split\n",
    "import os\n",
    "import zipfile\n",
    "\n",
    "# Define a function to load the CSV data and create a tf.data.Dataset object\n",
    "def create_dataset(csv_path, batch_size=32, buffer_size=1024, validation_split=0.2, shuffle=True, start=None):\n",
    "    # Load the CSV data\n",
    "    with open(csv_path) as f:\n",
    "        csv_reader = csv.reader(f)\n",
    "        header = next(csv_reader)\n",
    "        feature_names = header[start:-1]\n",
    "        label_name = header[-1]\n",
    "        features = []\n",
    "        labels = []\n",
    "        for row in csv_reader:\n",
    "            features.append([float(x) for x in row[start:-1]])\n",
    "            labels.append(float(row[-1]))\n",
    "    features = np.array(features)\n",
    "    labels = np.array(labels)\n",
    "\n",
    "    # Split the data into training and validation sets\n",
    "    split_idx = int(len(features) * (1.0 - validation_split))\n",
    "    train_features, train_labels = features[:split_idx], labels[:split_idx]\n",
    "    val_features, val_labels = features[split_idx:], labels[split_idx:]\n",
    "\n",
    "    # Create a tf.data.Dataset object for the training data\n",
    "    train_ds = tf.data.Dataset.from_tensor_slices((train_features, train_labels))\n",
    "    if shuffle:\n",
    "        train_ds = train_ds.shuffle(buffer_size=buffer_size)\n",
    "    train_ds = train_ds.batch(batch_size)\n",
    "\n",
    "    # Create a tf.data.Dataset object for the validation data\n",
    "    val_ds = tf.data.Dataset.from_tensor_slices((val_features, val_labels)).prefetch(tf.data.experimental.AUTOTUNE)\n",
    "    val_ds = val_ds.batch(batch_size).prefetch(tf.data.experimental.AUTOTUNE)\n",
    "\n",
    "    return train_ds, val_ds\n",
    "\n",
    "BASE_EPOCHS  = 20\n",
    "VAL_SPLIT = 0.2\n",
    "DIABETES_CSV_PATH = 'diabetes.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(DIABETES_CSV_PATH):\n",
    "    url = 'https://jrssbcrsefilesnait.blob.core.windows.net/3950data1/diabetes.csv'\n",
    "    d_path = tf.keras.utils.get_file(origin=url, extract=True, archive_format='auto')\n",
    "    print(d_path)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Simple and Small Example\n",
    "\n",
    "We can test the generator on a small file. \n",
    "\n",
    "<b>Note:</b> with small examples, we won't really see any advantage in terms of speed as we can probably just load the data into memory without concern, no matter what. This starts to matter more when dealing with large files, where the disk access time can actually add up. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "20/20 [==============================] - 2s 11ms/step - loss: 2.2207 - accuracy: 0.5896 - val_loss: 1.0937 - val_accuracy: 0.5195\n",
      "Epoch 2/10\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.8641 - accuracy: 0.6498 - val_loss: 0.7717 - val_accuracy: 0.6753\n",
      "Epoch 3/10\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.8923 - accuracy: 0.6026 - val_loss: 0.7729 - val_accuracy: 0.6234\n",
      "Epoch 4/10\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.7261 - accuracy: 0.6547 - val_loss: 0.6505 - val_accuracy: 0.7013\n",
      "Epoch 5/10\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.7052 - accuracy: 0.6743 - val_loss: 0.6388 - val_accuracy: 0.6494\n",
      "Epoch 6/10\n",
      "20/20 [==============================] - 0s 3ms/step - loss: 0.6043 - accuracy: 0.6775 - val_loss: 0.6155 - val_accuracy: 0.6623\n",
      "Epoch 7/10\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.6316 - accuracy: 0.6906 - val_loss: 0.6929 - val_accuracy: 0.6883\n",
      "Epoch 8/10\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.6072 - accuracy: 0.7134 - val_loss: 0.9397 - val_accuracy: 0.5260\n",
      "Epoch 9/10\n",
      "20/20 [==============================] - 0s 3ms/step - loss: 0.8786 - accuracy: 0.6270 - val_loss: 0.7748 - val_accuracy: 0.5844\n",
      "Epoch 10/10\n",
      "20/20 [==============================] - 0s 3ms/step - loss: 0.7991 - accuracy: 0.6580 - val_loss: 1.1130 - val_accuracy: 0.6623\n",
      "DS Training time: 2.1036951541900635 seconds\n"
     ]
    }
   ],
   "source": [
    "# Load the CSV data and create the tf.data.Dataset objects\n",
    "train_ds, val_ds = create_dataset(DIABETES_CSV_PATH)\n",
    "\n",
    "# Define the model\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Dense(64, activation='relu'),\n",
    "    tf.keras.layers.Dense(64, activation='relu'),\n",
    "    tf.keras.layers.Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer=tf.keras.optimizers.Adam(),\n",
    "              loss=tf.keras.losses.BinaryCrossentropy(),\n",
    "              metrics=\"accuracy\")\n",
    "\n",
    "# Fit the model to the data\n",
    "start = time.time()\n",
    "model.fit(train_ds, epochs=BASE_EPOCHS, validation_data=val_ds)\n",
    "end = time.time()\n",
    "print(\"DS Training time: {} seconds\".format(end - start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "20/20 [==============================] - 2s 17ms/step - loss: 1.5506 - accuracy: 0.5961 - val_loss: 0.8732 - val_accuracy: 0.6623\n",
      "Epoch 2/20\n",
      "20/20 [==============================] - 0s 6ms/step - loss: 0.8108 - accuracy: 0.6580 - val_loss: 0.6910 - val_accuracy: 0.5649\n",
      "Epoch 3/20\n",
      "20/20 [==============================] - 0s 5ms/step - loss: 0.6682 - accuracy: 0.6759 - val_loss: 0.6774 - val_accuracy: 0.5649\n",
      "Epoch 4/20\n",
      "20/20 [==============================] - 0s 6ms/step - loss: 0.6235 - accuracy: 0.7068 - val_loss: 0.8571 - val_accuracy: 0.4935\n",
      "Epoch 5/20\n",
      "20/20 [==============================] - 0s 5ms/step - loss: 0.6579 - accuracy: 0.6840 - val_loss: 0.8537 - val_accuracy: 0.5260\n",
      "Epoch 6/20\n",
      "20/20 [==============================] - 0s 5ms/step - loss: 0.7190 - accuracy: 0.6580 - val_loss: 0.7565 - val_accuracy: 0.6299\n",
      "Epoch 7/20\n",
      "20/20 [==============================] - 0s 13ms/step - loss: 0.6878 - accuracy: 0.6678 - val_loss: 0.7133 - val_accuracy: 0.6558\n",
      "Epoch 8/20\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.6797 - accuracy: 0.6889 - val_loss: 0.9723 - val_accuracy: 0.6753\n",
      "Epoch 9/20\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.7483 - accuracy: 0.6759 - val_loss: 0.7029 - val_accuracy: 0.6429\n",
      "Epoch 10/20\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.6203 - accuracy: 0.6954 - val_loss: 0.6319 - val_accuracy: 0.6818\n",
      "Epoch 11/20\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.6011 - accuracy: 0.6987 - val_loss: 0.6157 - val_accuracy: 0.6948\n",
      "Epoch 12/20\n",
      "20/20 [==============================] - 0s 3ms/step - loss: 0.5857 - accuracy: 0.7166 - val_loss: 0.6271 - val_accuracy: 0.6753\n",
      "Epoch 13/20\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.6084 - accuracy: 0.7182 - val_loss: 0.6857 - val_accuracy: 0.7143\n",
      "Epoch 14/20\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.6282 - accuracy: 0.7248 - val_loss: 0.5993 - val_accuracy: 0.7143\n",
      "Epoch 15/20\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.5427 - accuracy: 0.7427 - val_loss: 0.6894 - val_accuracy: 0.6623\n",
      "Epoch 16/20\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.5548 - accuracy: 0.7248 - val_loss: 0.6519 - val_accuracy: 0.6429\n",
      "Epoch 17/20\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.6068 - accuracy: 0.7020 - val_loss: 0.9789 - val_accuracy: 0.6494\n",
      "Epoch 18/20\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.7459 - accuracy: 0.6906 - val_loss: 0.7932 - val_accuracy: 0.6429\n",
      "Epoch 19/20\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.6927 - accuracy: 0.7052 - val_loss: 0.9345 - val_accuracy: 0.5844\n",
      "Epoch 20/20\n",
      "20/20 [==============================] - 0s 9ms/step - loss: 0.6713 - accuracy: 0.6824 - val_loss: 0.7041 - val_accuracy: 0.6364\n",
      "DS Training time: 3.7168478965759277 seconds\n"
     ]
    }
   ],
   "source": [
    "# Time dataframe for comparison\n",
    "df_small = pd.read_csv(DIABETES_CSV_PATH)\n",
    "df_small_y = df_small[\"Outcome\"]\n",
    "df_small_X = df_small.drop(columns=[\"Outcome\"])\n",
    "width = df_small_X.shape[1]\n",
    "# Define the model\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.InputLayer(input_shape=(width,)),\n",
    "    tf.keras.layers.Dense(64, activation='relu'),\n",
    "    tf.keras.layers.Dense(64, activation='relu'),\n",
    "    tf.keras.layers.Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "\n",
    "model.compile(optimizer=tf.keras.optimizers.Adam(),\n",
    "              loss=tf.keras.losses.BinaryCrossentropy(),\n",
    "              metrics=\"accuracy\")\n",
    "# Fit the model to the data\n",
    "start = time.time()\n",
    "model.fit(x=df_small_X, y=df_small_y, epochs=BASE_EPOCHS, validation_split=0.2)\n",
    "end = time.time()\n",
    "print(\"DS Training time: {} seconds\".format(end - start))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Larger Example\n",
    "\n",
    "We can download a larger file, and try it out. We ill also use the .cache() method to cache the data in memory, so that we don't have to reload it every time we run the code. This CSV file is roughly 150mb in size, so it is large enough to be noticable when we need to load the entire thing, but small enough to fit in memory. For most CSV data that we might encounter, this is probably a good approach - most systems can handle the memory demands of the CSV file size we might see. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/akeems/.keras/datasets/creditcard.csv\n"
     ]
    }
   ],
   "source": [
    "# Download the file\n",
    "\n",
    "zip_name = 'fraud.zip'\n",
    "if not os.path.exists(zip_name):\n",
    "    url = 'https://jrssbcrsefilesnait.blob.core.windows.net/3950data1/creditcard.csv'\n",
    "    zip_path = tf.keras.utils.get_file(origin=url, extract=True, archive_format='auto')\n",
    "    print(zip_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "7121/7121 [==============================] - 13s 2ms/step - loss: 0.0253 - accuracy: 0.9987 - val_loss: 0.0056 - val_accuracy: 0.9995\n",
      "Epoch 2/20\n",
      "7121/7121 [==============================] - 12s 2ms/step - loss: 0.0099 - accuracy: 0.9992 - val_loss: 0.0050 - val_accuracy: 0.9995\n",
      "Epoch 3/20\n",
      "7121/7121 [==============================] - 14s 2ms/step - loss: 0.0053 - accuracy: 0.9993 - val_loss: 0.0046 - val_accuracy: 0.9995\n",
      "Epoch 4/20\n",
      "7121/7121 [==============================] - 15s 2ms/step - loss: 0.0051 - accuracy: 0.9993 - val_loss: 0.0041 - val_accuracy: 0.9996\n",
      "Epoch 5/20\n",
      "7121/7121 [==============================] - 16s 2ms/step - loss: 0.0044 - accuracy: 0.9994 - val_loss: 0.0038 - val_accuracy: 0.9996\n",
      "Epoch 6/20\n",
      "7121/7121 [==============================] - 16s 2ms/step - loss: 0.0040 - accuracy: 0.9994 - val_loss: 0.0032 - val_accuracy: 0.9996\n",
      "Epoch 7/20\n",
      "7121/7121 [==============================] - 19s 3ms/step - loss: 0.0038 - accuracy: 0.9994 - val_loss: 0.0033 - val_accuracy: 0.9996\n",
      "Epoch 8/20\n",
      "7121/7121 [==============================] - 14s 2ms/step - loss: 0.0040 - accuracy: 0.9995 - val_loss: 0.0033 - val_accuracy: 0.9996\n",
      "Epoch 9/20\n",
      "7121/7121 [==============================] - 14s 2ms/step - loss: 0.0036 - accuracy: 0.9995 - val_loss: 0.0034 - val_accuracy: 0.9996\n",
      "Epoch 10/20\n",
      "7121/7121 [==============================] - 20s 3ms/step - loss: 0.0034 - accuracy: 0.9995 - val_loss: 0.0032 - val_accuracy: 0.9996\n",
      "Epoch 11/20\n",
      "7121/7121 [==============================] - 13s 2ms/step - loss: 0.0035 - accuracy: 0.9995 - val_loss: 0.0031 - val_accuracy: 0.9996\n",
      "Epoch 12/20\n",
      "7121/7121 [==============================] - 18s 2ms/step - loss: 0.0030 - accuracy: 0.9995 - val_loss: 0.0032 - val_accuracy: 0.9996\n",
      "Epoch 13/20\n",
      "7121/7121 [==============================] - 13s 2ms/step - loss: 0.0032 - accuracy: 0.9995 - val_loss: 0.0037 - val_accuracy: 0.9996\n",
      "Epoch 14/20\n",
      "7121/7121 [==============================] - 19s 3ms/step - loss: 0.0029 - accuracy: 0.9995 - val_loss: 0.0036 - val_accuracy: 0.9996\n",
      "Epoch 15/20\n",
      "7121/7121 [==============================] - 15s 2ms/step - loss: 0.0029 - accuracy: 0.9996 - val_loss: 0.0038 - val_accuracy: 0.9995\n",
      "Epoch 16/20\n",
      "7121/7121 [==============================] - 15s 2ms/step - loss: 0.0027 - accuracy: 0.9996 - val_loss: 0.0044 - val_accuracy: 0.9995\n",
      "Epoch 17/20\n",
      "7121/7121 [==============================] - 13s 2ms/step - loss: 0.0029 - accuracy: 0.9995 - val_loss: 0.0036 - val_accuracy: 0.9994\n",
      "Epoch 18/20\n",
      "7121/7121 [==============================] - 13s 2ms/step - loss: 0.0027 - accuracy: 0.9996 - val_loss: 0.0038 - val_accuracy: 0.9994\n",
      "Epoch 19/20\n",
      "7121/7121 [==============================] - 13s 2ms/step - loss: 0.0026 - accuracy: 0.9996 - val_loss: 0.0037 - val_accuracy: 0.9995\n",
      "Epoch 20/20\n",
      "7121/7121 [==============================] - 12s 2ms/step - loss: 0.0028 - accuracy: 0.9996 - val_loss: 0.0065 - val_accuracy: 0.9994\n",
      "Time to fit:  298.03627705574036\n"
     ]
    }
   ],
   "source": [
    "big_file = \"/Users/akeems/.keras/datasets/creditcard.csv\"\n",
    "#big_file = zip_path\n",
    "# Load the CSV data and create the tf.data.Dataset objects\n",
    "train_ds, val_ds = create_dataset(big_file, start=1)\n",
    "\n",
    "# Force all the data into memory, for faster training\n",
    "train_ds = train_ds.cache().prefetch(tf.data.experimental.AUTOTUNE)\n",
    "val_ds = val_ds.cache().prefetch(tf.data.experimental.AUTOTUNE)\n",
    "\n",
    "# Define the model\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Dense(64, activation='relu'),\n",
    "    tf.keras.layers.Dense(64, activation='relu'),\n",
    "    tf.keras.layers.Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer=tf.keras.optimizers.Adam(),\n",
    "              loss=tf.keras.losses.BinaryCrossentropy(),\n",
    "              metrics=\"accuracy\")\n",
    "\n",
    "# Fit the model to the data\n",
    "# time the fit\n",
    "\n",
    "start = time.time()\n",
    "model.fit(train_ds, epochs=BASE_EPOCHS, validation_data=val_ds)\n",
    "end = time.time()\n",
    "print(\"Time to fit: \", end - start)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dataframe for Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "7121/7121 [==============================] - 20s 3ms/step - loss: 11.9664 - accuracy: 0.9961 - val_loss: 5.7599 - val_accuracy: 0.9987\n",
      "Epoch 2/20\n",
      "7121/7121 [==============================] - 15s 2ms/step - loss: 1.7248 - accuracy: 0.9960 - val_loss: 0.1304 - val_accuracy: 0.9987\n",
      "Epoch 3/20\n",
      "7121/7121 [==============================] - 13s 2ms/step - loss: 0.0939 - accuracy: 0.9979 - val_loss: 0.0500 - val_accuracy: 0.9987\n",
      "Epoch 4/20\n",
      "7121/7121 [==============================] - 13s 2ms/step - loss: 0.1660 - accuracy: 0.9975 - val_loss: 0.0104 - val_accuracy: 0.9987\n",
      "Epoch 5/20\n",
      "7121/7121 [==============================] - 15s 2ms/step - loss: 0.0290 - accuracy: 0.9979 - val_loss: 0.0102 - val_accuracy: 0.9987\n",
      "Epoch 6/20\n",
      "7121/7121 [==============================] - 17s 2ms/step - loss: 0.0178 - accuracy: 0.9980 - val_loss: 0.0102 - val_accuracy: 0.9987\n",
      "Epoch 7/20\n",
      "7121/7121 [==============================] - 15s 2ms/step - loss: 0.0327 - accuracy: 0.9980 - val_loss: 0.0101 - val_accuracy: 0.9987\n",
      "Epoch 8/20\n",
      "7121/7121 [==============================] - 12s 2ms/step - loss: 0.0218 - accuracy: 0.9979 - val_loss: 0.0101 - val_accuracy: 0.9987\n",
      "Epoch 9/20\n",
      "7121/7121 [==============================] - 12s 2ms/step - loss: 0.0271 - accuracy: 0.9980 - val_loss: 0.0101 - val_accuracy: 0.9987\n",
      "Epoch 10/20\n",
      "7121/7121 [==============================] - 12s 2ms/step - loss: 0.0165 - accuracy: 0.9981 - val_loss: 0.0101 - val_accuracy: 0.9987\n",
      "Epoch 11/20\n",
      "7121/7121 [==============================] - 12s 2ms/step - loss: 0.0191 - accuracy: 0.9980 - val_loss: 0.0101 - val_accuracy: 0.9987\n",
      "Epoch 12/20\n",
      "7121/7121 [==============================] - 12s 2ms/step - loss: 0.0154 - accuracy: 0.9981 - val_loss: 0.0102 - val_accuracy: 0.9987\n",
      "Epoch 13/20\n",
      "7121/7121 [==============================] - 12s 2ms/step - loss: 0.0156 - accuracy: 0.9980 - val_loss: 0.0101 - val_accuracy: 0.9987\n",
      "Epoch 14/20\n",
      "7121/7121 [==============================] - 12s 2ms/step - loss: 0.0213 - accuracy: 0.9980 - val_loss: 0.0101 - val_accuracy: 0.9987\n",
      "Epoch 15/20\n",
      "7121/7121 [==============================] - 14s 2ms/step - loss: 0.0192 - accuracy: 0.9980 - val_loss: 0.0101 - val_accuracy: 0.9987\n",
      "Epoch 16/20\n",
      "7121/7121 [==============================] - 14s 2ms/step - loss: 0.0187 - accuracy: 0.9980 - val_loss: 0.0105 - val_accuracy: 0.9987\n",
      "Epoch 17/20\n",
      "7121/7121 [==============================] - 14s 2ms/step - loss: 0.0157 - accuracy: 0.9981 - val_loss: 0.0102 - val_accuracy: 0.9987\n",
      "Epoch 18/20\n",
      "7121/7121 [==============================] - 15s 2ms/step - loss: 0.0167 - accuracy: 0.9980 - val_loss: 0.0101 - val_accuracy: 0.9987\n",
      "Epoch 19/20\n",
      "7121/7121 [==============================] - 15s 2ms/step - loss: 0.0156 - accuracy: 0.9981 - val_loss: 0.0101 - val_accuracy: 0.9987\n",
      "Epoch 20/20\n",
      "7121/7121 [==============================] - 11s 2ms/step - loss: 0.0204 - accuracy: 0.9980 - val_loss: 0.0101 - val_accuracy: 0.9987\n",
      "Time to fit:  277.58421516418457\n"
     ]
    }
   ],
   "source": [
    "df_large = pd.read_csv(big_file)\n",
    "df_large_y = df_large[\"Class\"]\n",
    "df_large_X = df_large.drop(columns={\"Class\"})\n",
    "width = df_large_X.shape[1]\n",
    "# Define the model\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.InputLayer(input_shape=(width,)),\n",
    "    tf.keras.layers.Dense(64, activation='relu'),\n",
    "    tf.keras.layers.Dense(64, activation='relu'),\n",
    "    tf.keras.layers.Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer=tf.keras.optimizers.Adam(),\n",
    "                loss=tf.keras.losses.BinaryCrossentropy(),\n",
    "                metrics=\"accuracy\")\n",
    "\n",
    "# Fit the model to the data\n",
    "# time the fit\n",
    "start = time.time()\n",
    "model.fit(x=df_large_X, y=df_large_y, epochs=BASE_EPOCHS, validation_split=VAL_SPLIT)\n",
    "end = time.time()\n",
    "print(\"Time to fit: \", end - start)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataframe to Dataset\n",
    "\n",
    "If we have a dataframe we can convert it to a dataset using the from_tensor_slices() method. Manipulating the data in a dataframe is far easier, so we can prep in a df then convert to a dataset. The function below creates a dataset from a dataframe, long with a few of the other things we commonly want to do in our data prep. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_keras_dataset(df, target=\"target\", val_split=0.2, batch_size=32):\n",
    "    # Splitting the dataframe into training and validation sets\n",
    "    train_df, val_df = train_test_split(df, test_size=val_split, random_state=42)\n",
    "\n",
    "    # Extracting the target variable from the dataframes\n",
    "    train_y = train_df.pop(target)\n",
    "    val_y = val_df.pop(target)\n",
    "\n",
    "    # Converting the target variable to categorical if necessary\n",
    "    num_classes = len(train_y.unique())\n",
    "    if num_classes > 2:\n",
    "        train_y = to_categorical(train_y, num_classes)\n",
    "        val_y = to_categorical(val_y, num_classes)\n",
    "\n",
    "    # Creating a tf.data.Dataset for training and validation sets\n",
    "    train_ds = tf.data.Dataset.from_tensor_slices((train_df.values, train_y))\n",
    "    val_ds = tf.data.Dataset.from_tensor_slices((val_df.values, val_y))\n",
    "\n",
    "    # Shuffling and batching the datasets\n",
    "    batch_size = 32\n",
    "    train_ds = train_ds.shuffle(len(train_df)).batch(batch_size).prefetch(tf.data.experimental.AUTOTUNE)\n",
    "    val_ds = val_ds.batch(batch_size).prefetch(tf.data.experimental.AUTOTUNE)\n",
    "\n",
    "    return train_ds, val_ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "7121/7121 [==============================] - 17s 2ms/step - loss: 12.6899 - accuracy: 0.9962 - val_loss: 21.1445 - val_accuracy: 0.0127\n",
      "Epoch 2/20\n",
      "7121/7121 [==============================] - 14s 2ms/step - loss: 1.7131 - accuracy: 0.9965 - val_loss: 0.0473 - val_accuracy: 0.9984\n",
      "Epoch 3/20\n",
      "7121/7121 [==============================] - 14s 2ms/step - loss: 0.2483 - accuracy: 0.9977 - val_loss: 0.0230 - val_accuracy: 0.9987\n",
      "Epoch 4/20\n",
      "7121/7121 [==============================] - 15s 2ms/step - loss: 0.1106 - accuracy: 0.9980 - val_loss: 0.0154 - val_accuracy: 0.9986\n",
      "Epoch 5/20\n",
      "7121/7121 [==============================] - 14s 2ms/step - loss: 0.0330 - accuracy: 0.9980 - val_loss: 0.0127 - val_accuracy: 0.9983\n",
      "Epoch 6/20\n",
      "7121/7121 [==============================] - 14s 2ms/step - loss: 0.0196 - accuracy: 0.9983 - val_loss: 0.0127 - val_accuracy: 0.9983\n",
      "Epoch 7/20\n",
      "7121/7121 [==============================] - 16s 2ms/step - loss: 0.0235 - accuracy: 0.9981 - val_loss: 0.0127 - val_accuracy: 0.9983\n",
      "Epoch 8/20\n",
      "7121/7121 [==============================] - 14s 2ms/step - loss: 0.0802 - accuracy: 0.9979 - val_loss: 0.0127 - val_accuracy: 0.9983\n",
      "Epoch 9/20\n",
      "7121/7121 [==============================] - 14s 2ms/step - loss: 0.0248 - accuracy: 0.9981 - val_loss: 0.0127 - val_accuracy: 0.9983\n",
      "Epoch 10/20\n",
      "7121/7121 [==============================] - 16s 2ms/step - loss: 0.0167 - accuracy: 0.9981 - val_loss: 0.0127 - val_accuracy: 0.9983\n",
      "Epoch 11/20\n",
      "7121/7121 [==============================] - 17s 2ms/step - loss: 0.0187 - accuracy: 0.9981 - val_loss: 0.0127 - val_accuracy: 0.9983\n",
      "Epoch 12/20\n",
      "7121/7121 [==============================] - 17s 2ms/step - loss: 0.0142 - accuracy: 0.9983 - val_loss: 0.0127 - val_accuracy: 0.9983\n",
      "Epoch 13/20\n",
      "7121/7121 [==============================] - 20s 3ms/step - loss: 0.0162 - accuracy: 0.9983 - val_loss: 0.0128 - val_accuracy: 0.9982\n",
      "Epoch 14/20\n",
      "7121/7121 [==============================] - 18s 2ms/step - loss: 0.0189 - accuracy: 0.9981 - val_loss: 0.0128 - val_accuracy: 0.9983\n",
      "Epoch 15/20\n",
      "7121/7121 [==============================] - 17s 2ms/step - loss: 0.0309 - accuracy: 0.9977 - val_loss: 0.0130 - val_accuracy: 0.9983\n",
      "Epoch 16/20\n",
      "7121/7121 [==============================] - 14s 2ms/step - loss: 0.0265 - accuracy: 0.9982 - val_loss: 0.0130 - val_accuracy: 0.9983\n",
      "Epoch 17/20\n",
      "7121/7121 [==============================] - 15s 2ms/step - loss: 0.0211 - accuracy: 0.9981 - val_loss: 0.0127 - val_accuracy: 0.9983\n",
      "Epoch 18/20\n",
      "7121/7121 [==============================] - 15s 2ms/step - loss: 0.0173 - accuracy: 0.9981 - val_loss: 0.0131 - val_accuracy: 0.9983\n",
      "Epoch 19/20\n",
      "7121/7121 [==============================] - 17s 2ms/step - loss: 0.0174 - accuracy: 0.9983 - val_loss: 0.0128 - val_accuracy: 0.9983\n",
      "Epoch 20/20\n",
      "7121/7121 [==============================] - 14s 2ms/step - loss: 0.0217 - accuracy: 0.9981 - val_loss: 0.0127 - val_accuracy: 0.9983\n",
      "Time to fit:  312.27247500419617\n"
     ]
    }
   ],
   "source": [
    "train_ds_df, val_ds_df = get_keras_dataset(df_large, target=\"Class\", val_split=VAL_SPLIT, batch_size=64)\n",
    "\n",
    "# Define the model\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Dense(64, activation='relu'),\n",
    "    tf.keras.layers.Dense(64, activation='relu'),\n",
    "    tf.keras.layers.Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer=tf.keras.optimizers.Adam(),\n",
    "              loss=tf.keras.losses.BinaryCrossentropy(),\n",
    "              metrics=\"accuracy\")\n",
    "\n",
    "# Fit the model to the data\n",
    "# time the fit\n",
    "\n",
    "start = time.time()\n",
    "model.fit(train_ds_df, epochs=BASE_EPOCHS, validation_data=val_ds_df)\n",
    "end = time.time()\n",
    "print(\"Time to fit: \", end - start)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Polars DataFrame\n",
    "\n",
    "We can also use the faster and more efficient Polars DataFrame to load the data. This is a DataFrame that is written in Rust, and is much faster than Pandas. Polars dataframes aren't promised to be a one-to-one replacement for Pandas, but they are very similar, and can be used in most cases where Pandas is used with few, if any, changes.\n",
    "\n",
    "#### Polars Specifics\n",
    "\n",
    "Polars offers a fair bit of stuff for performance, as that is it's main selling point. Among them:\n",
    "<ul>\n",
    "<li> Low memory parameter - this will try to load the data in a way that uses less memory, but may be slower. </li>\n",
    "<li> Lazy execution - Polars has options to work lazily, which means that it won't actually do work like loading data until it is needed. </li>\n",
    "<li> Parallel execution - Polars can use multiple threads to do work, which can speed things up. </li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "import polars as pl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       ".pl-dataframe > thead > tr > th {\n",
       "  text-align: right;\n",
       "}\n",
       "</style>\n",
       "\n",
       "<table border=\"1\" class=\"dataframe pl-dataframe\">\n",
       "<small>shape: (5, 31)</small>\n",
       "<thead>\n",
       "<tr>\n",
       "<th>\n",
       "Time\n",
       "</th>\n",
       "<th>\n",
       "V1\n",
       "</th>\n",
       "<th>\n",
       "V2\n",
       "</th>\n",
       "<th>\n",
       "V3\n",
       "</th>\n",
       "<th>\n",
       "V4\n",
       "</th>\n",
       "<th>\n",
       "V5\n",
       "</th>\n",
       "<th>\n",
       "V6\n",
       "</th>\n",
       "<th>\n",
       "V7\n",
       "</th>\n",
       "<th>\n",
       "V8\n",
       "</th>\n",
       "<th>\n",
       "V9\n",
       "</th>\n",
       "<th>\n",
       "V10\n",
       "</th>\n",
       "<th>\n",
       "V11\n",
       "</th>\n",
       "<th>\n",
       "V12\n",
       "</th>\n",
       "<th>\n",
       "V13\n",
       "</th>\n",
       "<th>\n",
       "V14\n",
       "</th>\n",
       "<th>\n",
       "V15\n",
       "</th>\n",
       "<th>\n",
       "V16\n",
       "</th>\n",
       "<th>\n",
       "V17\n",
       "</th>\n",
       "<th>\n",
       "V18\n",
       "</th>\n",
       "<th>\n",
       "V19\n",
       "</th>\n",
       "<th>\n",
       "V20\n",
       "</th>\n",
       "<th>\n",
       "V21\n",
       "</th>\n",
       "<th>\n",
       "V22\n",
       "</th>\n",
       "<th>\n",
       "V23\n",
       "</th>\n",
       "<th>\n",
       "V24\n",
       "</th>\n",
       "<th>\n",
       "V25\n",
       "</th>\n",
       "<th>\n",
       "V26\n",
       "</th>\n",
       "<th>\n",
       "V27\n",
       "</th>\n",
       "<th>\n",
       "V28\n",
       "</th>\n",
       "<th>\n",
       "Amount\n",
       "</th>\n",
       "<th>\n",
       "Class\n",
       "</th>\n",
       "</tr>\n",
       "<tr>\n",
       "<td>\n",
       "i64\n",
       "</td>\n",
       "<td>\n",
       "f64\n",
       "</td>\n",
       "<td>\n",
       "f64\n",
       "</td>\n",
       "<td>\n",
       "f64\n",
       "</td>\n",
       "<td>\n",
       "f64\n",
       "</td>\n",
       "<td>\n",
       "f64\n",
       "</td>\n",
       "<td>\n",
       "f64\n",
       "</td>\n",
       "<td>\n",
       "f64\n",
       "</td>\n",
       "<td>\n",
       "f64\n",
       "</td>\n",
       "<td>\n",
       "f64\n",
       "</td>\n",
       "<td>\n",
       "f64\n",
       "</td>\n",
       "<td>\n",
       "f64\n",
       "</td>\n",
       "<td>\n",
       "f64\n",
       "</td>\n",
       "<td>\n",
       "f64\n",
       "</td>\n",
       "<td>\n",
       "f64\n",
       "</td>\n",
       "<td>\n",
       "f64\n",
       "</td>\n",
       "<td>\n",
       "f64\n",
       "</td>\n",
       "<td>\n",
       "f64\n",
       "</td>\n",
       "<td>\n",
       "f64\n",
       "</td>\n",
       "<td>\n",
       "f64\n",
       "</td>\n",
       "<td>\n",
       "f64\n",
       "</td>\n",
       "<td>\n",
       "f64\n",
       "</td>\n",
       "<td>\n",
       "f64\n",
       "</td>\n",
       "<td>\n",
       "f64\n",
       "</td>\n",
       "<td>\n",
       "f64\n",
       "</td>\n",
       "<td>\n",
       "f64\n",
       "</td>\n",
       "<td>\n",
       "f64\n",
       "</td>\n",
       "<td>\n",
       "f64\n",
       "</td>\n",
       "<td>\n",
       "f64\n",
       "</td>\n",
       "<td>\n",
       "f64\n",
       "</td>\n",
       "<td>\n",
       "i64\n",
       "</td>\n",
       "</tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr>\n",
       "<td>\n",
       "0\n",
       "</td>\n",
       "<td>\n",
       "-1.359807\n",
       "</td>\n",
       "<td>\n",
       "-0.072781\n",
       "</td>\n",
       "<td>\n",
       "2.536347\n",
       "</td>\n",
       "<td>\n",
       "1.378155\n",
       "</td>\n",
       "<td>\n",
       "-0.338321\n",
       "</td>\n",
       "<td>\n",
       "0.462388\n",
       "</td>\n",
       "<td>\n",
       "0.239599\n",
       "</td>\n",
       "<td>\n",
       "0.098698\n",
       "</td>\n",
       "<td>\n",
       "0.363787\n",
       "</td>\n",
       "<td>\n",
       "0.090794\n",
       "</td>\n",
       "<td>\n",
       "-0.5516\n",
       "</td>\n",
       "<td>\n",
       "-0.617801\n",
       "</td>\n",
       "<td>\n",
       "-0.99139\n",
       "</td>\n",
       "<td>\n",
       "-0.311169\n",
       "</td>\n",
       "<td>\n",
       "1.468177\n",
       "</td>\n",
       "<td>\n",
       "-0.470401\n",
       "</td>\n",
       "<td>\n",
       "0.207971\n",
       "</td>\n",
       "<td>\n",
       "0.025791\n",
       "</td>\n",
       "<td>\n",
       "0.403993\n",
       "</td>\n",
       "<td>\n",
       "0.251412\n",
       "</td>\n",
       "<td>\n",
       "-0.018307\n",
       "</td>\n",
       "<td>\n",
       "0.277838\n",
       "</td>\n",
       "<td>\n",
       "-0.110474\n",
       "</td>\n",
       "<td>\n",
       "0.066928\n",
       "</td>\n",
       "<td>\n",
       "0.128539\n",
       "</td>\n",
       "<td>\n",
       "-0.189115\n",
       "</td>\n",
       "<td>\n",
       "0.133558\n",
       "</td>\n",
       "<td>\n",
       "-0.021053\n",
       "</td>\n",
       "<td>\n",
       "149.62\n",
       "</td>\n",
       "<td>\n",
       "0\n",
       "</td>\n",
       "</tr>\n",
       "<tr>\n",
       "<td>\n",
       "0\n",
       "</td>\n",
       "<td>\n",
       "1.191857\n",
       "</td>\n",
       "<td>\n",
       "0.266151\n",
       "</td>\n",
       "<td>\n",
       "0.16648\n",
       "</td>\n",
       "<td>\n",
       "0.448154\n",
       "</td>\n",
       "<td>\n",
       "0.060018\n",
       "</td>\n",
       "<td>\n",
       "-0.082361\n",
       "</td>\n",
       "<td>\n",
       "-0.078803\n",
       "</td>\n",
       "<td>\n",
       "0.085102\n",
       "</td>\n",
       "<td>\n",
       "-0.255425\n",
       "</td>\n",
       "<td>\n",
       "-0.166974\n",
       "</td>\n",
       "<td>\n",
       "1.612727\n",
       "</td>\n",
       "<td>\n",
       "1.065235\n",
       "</td>\n",
       "<td>\n",
       "0.489095\n",
       "</td>\n",
       "<td>\n",
       "-0.143772\n",
       "</td>\n",
       "<td>\n",
       "0.635558\n",
       "</td>\n",
       "<td>\n",
       "0.463917\n",
       "</td>\n",
       "<td>\n",
       "-0.114805\n",
       "</td>\n",
       "<td>\n",
       "-0.183361\n",
       "</td>\n",
       "<td>\n",
       "-0.145783\n",
       "</td>\n",
       "<td>\n",
       "-0.069083\n",
       "</td>\n",
       "<td>\n",
       "-0.225775\n",
       "</td>\n",
       "<td>\n",
       "-0.638672\n",
       "</td>\n",
       "<td>\n",
       "0.101288\n",
       "</td>\n",
       "<td>\n",
       "-0.339846\n",
       "</td>\n",
       "<td>\n",
       "0.16717\n",
       "</td>\n",
       "<td>\n",
       "0.125895\n",
       "</td>\n",
       "<td>\n",
       "-0.008983\n",
       "</td>\n",
       "<td>\n",
       "0.014724\n",
       "</td>\n",
       "<td>\n",
       "2.69\n",
       "</td>\n",
       "<td>\n",
       "0\n",
       "</td>\n",
       "</tr>\n",
       "<tr>\n",
       "<td>\n",
       "1\n",
       "</td>\n",
       "<td>\n",
       "-1.358354\n",
       "</td>\n",
       "<td>\n",
       "-1.340163\n",
       "</td>\n",
       "<td>\n",
       "1.773209\n",
       "</td>\n",
       "<td>\n",
       "0.37978\n",
       "</td>\n",
       "<td>\n",
       "-0.503198\n",
       "</td>\n",
       "<td>\n",
       "1.800499\n",
       "</td>\n",
       "<td>\n",
       "0.791461\n",
       "</td>\n",
       "<td>\n",
       "0.247676\n",
       "</td>\n",
       "<td>\n",
       "-1.514654\n",
       "</td>\n",
       "<td>\n",
       "0.207643\n",
       "</td>\n",
       "<td>\n",
       "0.624501\n",
       "</td>\n",
       "<td>\n",
       "0.066084\n",
       "</td>\n",
       "<td>\n",
       "0.717293\n",
       "</td>\n",
       "<td>\n",
       "-0.165946\n",
       "</td>\n",
       "<td>\n",
       "2.345865\n",
       "</td>\n",
       "<td>\n",
       "-2.890083\n",
       "</td>\n",
       "<td>\n",
       "1.109969\n",
       "</td>\n",
       "<td>\n",
       "-0.121359\n",
       "</td>\n",
       "<td>\n",
       "-2.261857\n",
       "</td>\n",
       "<td>\n",
       "0.52498\n",
       "</td>\n",
       "<td>\n",
       "0.247998\n",
       "</td>\n",
       "<td>\n",
       "0.771679\n",
       "</td>\n",
       "<td>\n",
       "0.909412\n",
       "</td>\n",
       "<td>\n",
       "-0.689281\n",
       "</td>\n",
       "<td>\n",
       "-0.327642\n",
       "</td>\n",
       "<td>\n",
       "-0.139097\n",
       "</td>\n",
       "<td>\n",
       "-0.055353\n",
       "</td>\n",
       "<td>\n",
       "-0.059752\n",
       "</td>\n",
       "<td>\n",
       "378.66\n",
       "</td>\n",
       "<td>\n",
       "0\n",
       "</td>\n",
       "</tr>\n",
       "<tr>\n",
       "<td>\n",
       "1\n",
       "</td>\n",
       "<td>\n",
       "-0.966272\n",
       "</td>\n",
       "<td>\n",
       "-0.185226\n",
       "</td>\n",
       "<td>\n",
       "1.792993\n",
       "</td>\n",
       "<td>\n",
       "-0.863291\n",
       "</td>\n",
       "<td>\n",
       "-0.010309\n",
       "</td>\n",
       "<td>\n",
       "1.247203\n",
       "</td>\n",
       "<td>\n",
       "0.237609\n",
       "</td>\n",
       "<td>\n",
       "0.377436\n",
       "</td>\n",
       "<td>\n",
       "-1.387024\n",
       "</td>\n",
       "<td>\n",
       "-0.054952\n",
       "</td>\n",
       "<td>\n",
       "-0.226487\n",
       "</td>\n",
       "<td>\n",
       "0.178228\n",
       "</td>\n",
       "<td>\n",
       "0.507757\n",
       "</td>\n",
       "<td>\n",
       "-0.287924\n",
       "</td>\n",
       "<td>\n",
       "-0.631418\n",
       "</td>\n",
       "<td>\n",
       "-1.059647\n",
       "</td>\n",
       "<td>\n",
       "-0.684093\n",
       "</td>\n",
       "<td>\n",
       "1.965775\n",
       "</td>\n",
       "<td>\n",
       "-1.232622\n",
       "</td>\n",
       "<td>\n",
       "-0.208038\n",
       "</td>\n",
       "<td>\n",
       "-0.1083\n",
       "</td>\n",
       "<td>\n",
       "0.005274\n",
       "</td>\n",
       "<td>\n",
       "-0.190321\n",
       "</td>\n",
       "<td>\n",
       "-1.175575\n",
       "</td>\n",
       "<td>\n",
       "0.647376\n",
       "</td>\n",
       "<td>\n",
       "-0.221929\n",
       "</td>\n",
       "<td>\n",
       "0.062723\n",
       "</td>\n",
       "<td>\n",
       "0.061458\n",
       "</td>\n",
       "<td>\n",
       "123.5\n",
       "</td>\n",
       "<td>\n",
       "0\n",
       "</td>\n",
       "</tr>\n",
       "<tr>\n",
       "<td>\n",
       "2\n",
       "</td>\n",
       "<td>\n",
       "-1.158233\n",
       "</td>\n",
       "<td>\n",
       "0.877737\n",
       "</td>\n",
       "<td>\n",
       "1.548718\n",
       "</td>\n",
       "<td>\n",
       "0.403034\n",
       "</td>\n",
       "<td>\n",
       "-0.407193\n",
       "</td>\n",
       "<td>\n",
       "0.095921\n",
       "</td>\n",
       "<td>\n",
       "0.592941\n",
       "</td>\n",
       "<td>\n",
       "-0.270533\n",
       "</td>\n",
       "<td>\n",
       "0.817739\n",
       "</td>\n",
       "<td>\n",
       "0.753074\n",
       "</td>\n",
       "<td>\n",
       "-0.822843\n",
       "</td>\n",
       "<td>\n",
       "0.538196\n",
       "</td>\n",
       "<td>\n",
       "1.345852\n",
       "</td>\n",
       "<td>\n",
       "-1.11967\n",
       "</td>\n",
       "<td>\n",
       "0.175121\n",
       "</td>\n",
       "<td>\n",
       "-0.451449\n",
       "</td>\n",
       "<td>\n",
       "-0.237033\n",
       "</td>\n",
       "<td>\n",
       "-0.038195\n",
       "</td>\n",
       "<td>\n",
       "0.803487\n",
       "</td>\n",
       "<td>\n",
       "0.408542\n",
       "</td>\n",
       "<td>\n",
       "-0.009431\n",
       "</td>\n",
       "<td>\n",
       "0.798278\n",
       "</td>\n",
       "<td>\n",
       "-0.137458\n",
       "</td>\n",
       "<td>\n",
       "0.141267\n",
       "</td>\n",
       "<td>\n",
       "-0.20601\n",
       "</td>\n",
       "<td>\n",
       "0.502292\n",
       "</td>\n",
       "<td>\n",
       "0.219422\n",
       "</td>\n",
       "<td>\n",
       "0.215153\n",
       "</td>\n",
       "<td>\n",
       "69.99\n",
       "</td>\n",
       "<td>\n",
       "0\n",
       "</td>\n",
       "</tr>\n",
       "</tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "shape: (5, 31)\n",
       "┌──────┬───────────┬───────────┬──────────┬─────┬───────────┬───────────┬────────┬───────┐\n",
       "│ Time ┆ V1        ┆ V2        ┆ V3       ┆ ... ┆ V27       ┆ V28       ┆ Amount ┆ Class │\n",
       "│ ---  ┆ ---       ┆ ---       ┆ ---      ┆     ┆ ---       ┆ ---       ┆ ---    ┆ ---   │\n",
       "│ i64  ┆ f64       ┆ f64       ┆ f64      ┆     ┆ f64       ┆ f64       ┆ f64    ┆ i64   │\n",
       "╞══════╪═══════════╪═══════════╪══════════╪═════╪═══════════╪═══════════╪════════╪═══════╡\n",
       "│ 0    ┆ -1.359807 ┆ -0.072781 ┆ 2.536347 ┆ ... ┆ 0.133558  ┆ -0.021053 ┆ 149.62 ┆ 0     │\n",
       "│ 0    ┆ 1.191857  ┆ 0.266151  ┆ 0.16648  ┆ ... ┆ -0.008983 ┆ 0.014724  ┆ 2.69   ┆ 0     │\n",
       "│ 1    ┆ -1.358354 ┆ -1.340163 ┆ 1.773209 ┆ ... ┆ -0.055353 ┆ -0.059752 ┆ 378.66 ┆ 0     │\n",
       "│ 1    ┆ -0.966272 ┆ -0.185226 ┆ 1.792993 ┆ ... ┆ 0.062723  ┆ 0.061458  ┆ 123.5  ┆ 0     │\n",
       "│ 2    ┆ -1.158233 ┆ 0.877737  ┆ 1.548718 ┆ ... ┆ 0.219422  ┆ 0.215153  ┆ 69.99  ┆ 0     │\n",
       "└──────┴───────────┴───────────┴──────────┴─────┴───────────┴───────────┴────────┴───────┘"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Read file at zip path into a polars dataframe\n",
    "df_polar = pl.read_csv(zip_path, ignore_errors=True, low_memory=True)\n",
    "df_polar.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using Polars\n",
    "\n",
    "Polars doesn't have the same native support in TensorFlow as Pandas does, so we need to convert it to a Pandas dataframe or an array to feed it into any models. One thing that may be useful with Polars would be to split a very large csv into multiple smallers ones, that could then be loaded one at a time. Something like the function below could be adapted to load a csv into a Polars dataframe, do whatever data manipulation is needed, then write it out to several smaller csv files. The make_csv_dataset is able to natively read in multiple csv files. \n",
    "\n",
    "<b>Note:</b> If we were actually doing something like this, it is likely easier to do a train-validation-test split as we write the output into different subfolders. Manipulating data is easier in a dataframe than a dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 0\n",
    "#Shuffle data. Takes a peak of 2x memory to do so. \n",
    "df_polar = df_polar.sample(frac=1.0)\n",
    "for frame in df_polar.iter_slices(n_rows=10000):\n",
    "    record_batch = frame\n",
    "    fname = \"polar_out/fraud_{}.csv\".format(i)\n",
    "    record_batch.write_csv(fname)\n",
    "    i += 1"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Read Folder\n",
    "\n",
    "We can create datasets from a folder of csv files. This is useful if we have a large csv file that we have split into multiple smaller ones. We can utilize any of the tuning things like cache and batch size to control the memory usage. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "split_ds = tf.data.experimental.make_csv_dataset(\n",
    "            file_pattern = \"polar_out/*.csv\",\n",
    "            batch_size=64, \n",
    "            num_epochs=BASE_EPOCHS,\n",
    "            num_parallel_reads=20,\n",
    "            shuffle_buffer_size=10000)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml3950",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
